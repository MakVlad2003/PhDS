{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mt3SYsws89wu"
   },
   "source": [
    "# ML в Биологии\n",
    "## 9. Unsupervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QMpFPIRX89w3"
   },
   "outputs": [],
   "source": [
    "import numpy                as np\n",
    "import matplotlib.pyplot    as plt\n",
    "import matplotlib.cm        as cm\n",
    "import pandas               as pd\n",
    "import ipywidgets           as widgets\n",
    "from sklearn.decomposition  import PCA\n",
    "import seaborn              as sns\n",
    "import numpy                as np\n",
    "from tqdm.notebook          import tqdm\n",
    "\n",
    "from sklearn.cluster import \\\n",
    "    KMeans, \\\n",
    "    AgglomerativeClustering, \\\n",
    "    DBSCAN\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QApz5dZD89w4"
   },
   "source": [
    "### Задача 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "euJa96Qp89w4"
   },
   "source": [
    "Рассмотрим датасет [**Leaf Classification**](https://www.kaggle.com/c/leaf-classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbvglBeD89w5"
   },
   "source": [
    "Данные содержат 1584 изображений образцов листьев (16 изображений для 99 видов). Размер некоторых изображений изменен, в результате чего все изображения имеют одинаковый размер $170×250$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DPA-_cbc89w5"
   },
   "source": [
    "1. Скачайте данные с сайта."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xagzypM89w6"
   },
   "source": [
    "2. Загрузим все изображения с помощью `plt.imread` и визуализируем некоторые из них. Каждое изображение — матрица размера $170×250$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OBZchh2f8EFp",
    "outputId": "5976afc0-ba29-4ea2-f3ba-a75f67c440e7"
   },
   "outputs": [],
   "source": [
    "!unzip /content/scaled_images.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 767,
     "referenced_widgets": [
      "ed107a17160345059ab7ba99c5426e0c",
      "610803b3726c477db07cca378116a62c",
      "35d417baced04de983001363e2a09408",
      "4287ae49a099427081f5df5f78496fab",
      "c8de90bd7cd648e98b33e6c98d22b003",
      "7a3f6b32a6da4072ad48215546227ac0",
      "e043ec2b4a2d4d9ab5be324b3392d78f",
      "a659d25445a846e88ead595418f44083",
      "56a780f87559484b8df476a06511f3dc",
      "515616268644414d9c7ab632a2c88d58",
      "110a5f05084f47e0be34709906a3f1e4"
     ]
    },
    "id": "5CSOjWIL89w6",
    "outputId": "ba14b291-5067-46e6-af0d-5a1ad1193119"
   },
   "outputs": [],
   "source": [
    "n_images = 1584\n",
    "im_size = (170, 250)\n",
    "\n",
    "X = []\n",
    "for i in tqdm(range(n_images)):\n",
    "    new_img = plt.imread('scaled_images/' + str(i + 1) + '.jpg')\n",
    "    X.append(new_img)\n",
    "\n",
    "X = np.array(X)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(9):\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(X[i], cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TsKdOC3G89w7"
   },
   "source": [
    "Сделаем также так, чтобы каждое изображение было представлено не матрицей, а одним вектором из всех пикселей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dc2NPV9489w7"
   },
   "outputs": [],
   "source": [
    "X = X.reshape(len(X), -1)\n",
    "assert(X.shape ==(1584, 42500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnAzzYGt89w8"
   },
   "source": [
    "3. В файле `train_labels.csv` указаны номера образцов листьев, которые относятся к обучающей части данных, а также их виды."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 677
    },
    "id": "WAiAMG_j89w8",
    "outputId": "fec90d82-b1e4-44ef-b5e2-03a07866f515"
   },
   "outputs": [],
   "source": [
    "labels = pd.read_csv('train_labels.csv')\n",
    "labels.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PG6pFxSV89w8"
   },
   "source": [
    "Разделите данные на обучающую и тестовую часть."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UXrJhgv_7dGo"
   },
   "outputs": [],
   "source": [
    "train_ids = np.array(labels.id)\n",
    "test_ids = np.array(list(set(np.arange(1, len(X) + 1)) - set(train_ids)))\n",
    "\n",
    "X_train, X_test = X[train_ids - 1], X[test_ids - 1]\n",
    "y_train = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2FvLTagc89w9"
   },
   "source": [
    "Отсортируем теперь по алфавиту названия видов и построим отображение строки в индекс."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vAy6RTCj89w9"
   },
   "outputs": [],
   "source": [
    "species_names = sorted(np.unique(labels.species))\n",
    "name_to_ind = dict([(name, i) for (i, name) in enumerate(species_names)])\n",
    "labels.species = labels.species.map(name_to_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TuCR2ion89w9"
   },
   "source": [
    "4. На обучающей части данных постройте 30 главных компонент. Какую долю дисперсии данных они объясняют? Какую долю дисперсии объясняет каждая компонента отдельно? Постройте график доли объясненной дисперсии (зависимость доли от номера компоненты). Сделайте вывод.\n",
    "\n",
    "    * **Пояснение**: доля объясненной дисперсии - это показатель, характеризующий какую долю от общей дисперсии в данных объясняет данная компонента. Почитайте в документации метода главных компонент о [`explained_variance_ratio_`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#:~:text=in%20version%200.18.-,explained_variance_ratio_,-ndarray%20of%20shape).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-k6y0Zvd89w-"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "pca = PCA(30)\n",
    "X_pca = pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "id": "NGzAl9kV8u6o",
    "outputId": "950586f3-a53a-469d-9a17-668d4a6496c2"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 5))\n",
    "\n",
    "plt.plot(np.arange(1, 31), np.cumsum(pca.explained_variance_ratio_))\n",
    "\n",
    "plt.title(\"Суммарная доля объяснённой дисперсии\")\n",
    "plt.xlabel(\"Number of components\")\n",
    "plt.ylabel(\"Cumulative explained_variance_ratio\")\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "np.cumsum(pca.explained_variance_ratio_)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "DAeBoD4t89EU",
    "outputId": "1ac8ff99-f504-4fb3-bf9f-98102114ed94"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 5))\n",
    "\n",
    "plt.plot(np.arange(1, 31), pca.explained_variance_ratio_)\n",
    "\n",
    "plt.title(\"Доля объяснённой дисперсии каждой компоненты\")\n",
    "plt.xlabel(\"Number of component\")\n",
    "plt.ylabel(\"Explained_variance_ratio\")\n",
    "\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qXm3KWxPaUH8"
   },
   "source": [
    "**Вывод:**\n",
    "\n",
    "Видим, что суммарно первые 30 компонент описывают примерно 80% всей дисперсии.\n",
    "\n",
    "Наибольший вклад вносит первая компонента. После 10-й компоненты вклад каждой новой стремится к нулю."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_coOIDy89w-"
   },
   "source": [
    "5. Визуализируйте главные компоненты: покажите, какие картинки из себя представляют главные компоненты. Для этого перейдите обратно из представления изображения в виде одного длинного вектора к матрице. Можете ли вы их как-то охарактеризовать?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "102UmoMwcE2o",
    "outputId": "eaae6a6c-d10c-46c9-b137-24eb482994eb"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(20, 20), ncols=5, nrows=6)\n",
    "for i in range(6):\n",
    "    for j in range(5):\n",
    "        axs[i][j].imshow(pca.components_[5*i + j].reshape(170, 250))\n",
    "        axs[i][j].set_title(f\"Component №{5*i + j + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qruXTAFhicOe"
   },
   "source": [
    "**Вывод:**\n",
    "\n",
    "Для каждой компоненты видно некое усреднение всех форм листа с преобладанием одной из форм: какие-то горизонтально вытянуты, какие-то вертикально, какие-то звёздчатой формы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EY_dTqEw89w-"
   },
   "source": [
    "6. Визуализируйте обучающую часть данных в проекции на две первых главных компоненты. Цвет точки должен соответствовать виду образца. Используйте `cmap=’Set1’` во избежание градации цвета по номеру вида. Наблюдаются ли какие-либо закономерности?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "xd4PqhD2jQf0",
    "outputId": "46873ca9-945f-4b74-e6ff-9ee8c9709e66"
   },
   "outputs": [],
   "source": [
    "pc1, pc2 = X_pca[:, 0], X_pca[:, 1]\n",
    "plt.scatter(pc1, pc2, c=labels.species, cmap='Set1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UiRGZ8RSi0jO"
   },
   "source": [
    "**Вывод:**\n",
    "\n",
    "Какие-то закономерности видны, но сомнительно...\n",
    "\n",
    "Красные точечки явно примерно на одной линии, остальные тоже образуют условно связные области. Но никаких красивых кластеров."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CeW-4QLD89w_"
   },
   "source": [
    "7. По проекциям данных на первые 30 главных компонент обучите многоклассовую классификацию.\n",
    "\n",
    "    Используйте любую модель классификации, рассмотренную на нашем курсе.\n",
    "\n",
    "    Разделите данные на тренировочную и валидационную выборки и проверьте качество этой модели по метрике accuracy. Так как метки классов известны только для части данных, используйте только их. Сравните с результатом без применения PCA. Сделайте выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tyr7dlBW9qKv"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score as accuracy\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nUAFFSEV7dGq",
    "outputId": "a3fe0ed5-621a-4f3a-b202-78c338e4b7e1"
   },
   "outputs": [],
   "source": [
    "train_ftrs, test_ftrs, train_lbls, test_lbls = train_test_split(X_pca, labels.species, test_size=0.25)\n",
    "\n",
    "lr = RandomForestClassifier()\n",
    "lr.fit(train_ftrs, train_lbls)\n",
    "preds = lr.predict(test_ftrs)\n",
    "\n",
    "print(\"Accuracy with PCA: \", accuracy(test_lbls, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gDdY_eRX9ySM",
    "outputId": "170bb2b4-4ab2-4714-e83c-b7ab4048ee9d"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "train_ftrs, test_ftrs, train_lbls, test_lbls = train_test_split(X_train, labels.species, test_size=0.25)\n",
    "\n",
    "X_s = scaler.fit_transform(train_ftrs)\n",
    "X_ts = scaler.transform(test_ftrs)\n",
    "X_s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z4s4mf2N9z5F",
    "outputId": "bcb1533b-3a67-44c0-fb3e-aa5468311683"
   },
   "outputs": [],
   "source": [
    "lr = RandomForestClassifier()\n",
    "lr.fit(X_s, train_lbls)\n",
    "preds = lr.predict(X_ts)\n",
    "\n",
    "print(\"Accuracy without PCA: \",accuracy(test_lbls, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ht_h0YS396Qv"
   },
   "source": [
    "**Вывод:**\n",
    "\n",
    "Когда у нас размерность 42500, PCA явно поможет. И он помог. Метрика модели на главных компонентах выше, чем просто на исходном датасете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "SM6VW3fW-CKv",
    "outputId": "e7697076-f6d2-4cca-93a3-cd5acfb2fd61"
   },
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C42WDUB07dGq"
   },
   "source": [
    "### Задача 2\n",
    "\n",
    "В кишечнике человека живут триллионы микроорганизмов. Они могут влиять на совершенно различные процессы в организме. Однако на данный момент мы находимся лишь в самом начале пути изучения  механизмов их взаимодействия с организмом хозяина. Сейчас влияние микробиоты изучают во многих сферах медицины: в онкологии, психиатрии...\n",
    "\n",
    "Состави микробиоты у каждого человека свой. Однако важна не только похожесть состава, но и функции тех или иных организмов, в частности белки, которые они производят. Весьма много информации могут нам дать данные [метагеномного секвенирования](https://www.news-medical.net/life-sciences/Shotgun-Metagenomic-Sequencing.aspx). Этот тип секвенирования позволяет получать короткие последовательности ДНК от всех организмов в образце вместе, то есть не разделяя ДНК по организмам. Использование метагеномного секвнирования помогло обнаружить, насколько распространены гены антибиотиков в наших кишечных бактериях.\n",
    "\n",
    "На данный момент мы остановимся лишь на изучении того, какие организмы живут в кишечнике различных людей, а не на их функциях. Эти данные можно получить из данных метагеномного секвенирования, рассмотрев различные последовательности ДНК, соответствующие одной из субъединиц рибосомы. Оказывается, что по эти последовательности уникальны у каждого вида, и таким образом легко понять, какие микроорганизмы присутствуют в образце.\n",
    "\n",
    "Скачайте [датасет](https://www.kaggle.com/antaresnyc/human-metagenomics?select=abundance_stoolsubset.csv). Мы будем использовать файл `abundance_stoolsubset.csv`, в котором лежат уже предобработанные данные о том, ДНК каких организмов обнаружена в образцах микробиоты кишечника и в каком количестве.\n",
    "\n",
    "Данные были взяты из статьи\n",
    "[\"Machine Learning Meta-analysis of Large Metagenomic Datasets: Tools and Biological Insights\"\n",
    "](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004977) Pasolli et al.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WjuidWmm-jwd",
    "outputId": "f454890f-7487-45ab-dbe6-e71d0e9a04c0"
   },
   "outputs": [],
   "source": [
    "!unzip /content/archive.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290
    },
    "id": "CmzPnqG-7dGq",
    "outputId": "58264d00-b53f-41aa-e748-0c2f5c7eadf7"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"abundance_stoolsubset.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pZ068s17dGq"
   },
   "source": [
    "В данных есть много метаданных об образце. Их мы использовать не будем, так как хотим сравнивать людей по микроорганизмам.\n",
    "\n",
    "Метаданные занимают первые 210 колонок. Удалите их.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "id": "FGm-6-hi7dGq",
    "outputId": "37742cdb-46a4-4348-866c-2f72a65f265c"
   },
   "outputs": [],
   "source": [
    "data_clean = data.iloc[:, 210:]\n",
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qs5GZ5tO7dGq"
   },
   "source": [
    "Если посмотреть на данные, то можно заметить, что учитываются как количества ДНК организмов каждого вида, так и более высших иерархических единиц. Таким образом, в данных присутствует избыточность, так как некоторые столбцы просто являются линейной комбинацией друг друга.\n",
    "\n",
    "Снизьте размерность с помощью PCA. Подберите оптимальное количество главных компонент."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lGZja6KR7dGq",
    "outputId": "e70d4e94-b2dc-41a2-9c9c-bacb4ce17326"
   },
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(data_clean)\n",
    "\n",
    "# Подбираем оптимальное количество главных компонент\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = explained_variance.cumsum()\n",
    "\n",
    "# Находим оптимальное количество компонент, которые объясняют > 90% дисперсии\n",
    "optimal_components = (cumulative_variance >= 0.9).argmax() + 1\n",
    "print(f\"Оптимальное количество главных компонент: {optimal_components}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3BjNisEI7dGr"
   },
   "source": [
    "Визуализируйте данные с помощью t-SNE и UMAP. Используйте особенности методов. Не забудьте пояснить свое решение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "id": "bMYs1LYgAbG3",
    "outputId": "7d42bc3f-2c1f-40c7-c88a-d9b7216eabe0"
   },
   "outputs": [],
   "source": [
    "!pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wpPWM5k27dGr",
    "outputId": "b6fc954c-8a3a-40c5-9473-8d1bd79f416d"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from umap.umap_ import UMAP\n",
    "sns.set_style('white')\n",
    "\n",
    "X_tsne = TSNE(n_components=2, random_state=42).fit_transform(X_pca[:, :optimal_components])\n",
    "X_umap = UMAP(n_components=2, random_state=42).fit_transform(X_pca[:, :optimal_components])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SYMVzkG-7dGr"
   },
   "outputs": [],
   "source": [
    "def draw_umap_tsne(figsize, c=None, outliers=np.zeros(X_tsne.shape[0], dtype=bool)):\n",
    "    \"\"\"Функция, строящая проекции данных на 2 компоненты, получающиеся в результате применения UMAP\n",
    "\n",
    "    Args:\n",
    "        figsize: размер картинки\n",
    "        c: метки для отображения точек в цвете.\n",
    "        outliers: выбросы, если таковые имеются\n",
    "    \"\"\"\n",
    "    # t-SNE\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(X_tsne[~outliers, 0], X_tsne[~outliers, 1], c=c, cmap='viridis', s=30)  # Объекты, не являющиеся выбросами\n",
    "    plt.scatter(X_tsne[outliers, 0], X_tsne[outliers, 1], color='r', s=30, label='Outliers')  # Отдельно рисуем выбросы, если такие есть\n",
    "    plt.xlabel('Координата 1')\n",
    "    plt.ylabel('Координата 2')\n",
    "    plt.title('TSNE: Данные в латентном пространстве')\n",
    "\n",
    "    # UMAP\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(X_umap[~outliers, 0], X_umap[~outliers, 1], c=c, cmap='viridis', s=30)  # Объекты, не являющиеся выбросами\n",
    "    plt.scatter(X_umap[outliers, 0], X_umap[outliers, 1], color='r', s=300, label='Outliers')  # Отдельно рисуем выбросы, если такие есть\n",
    "    plt.xlabel('Координата 1')\n",
    "    plt.ylabel('Координата 2')\n",
    "    plt.title('UMAP: Данные в латентном пространстве')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 605
    },
    "id": "N0pbrCMG7dGr",
    "outputId": "b162effe-1171-424f-a9e2-9c34e05c1b29"
   },
   "outputs": [],
   "source": [
    "draw_umap_tsne(figsize=(20, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AM-VTnBA7dGr"
   },
   "source": [
    "Теперь перейдем к кластеризации. Попробуйте кластеризовать данные в исходном пространстве и в пространстве после применения PCA с помощью DBSCAN, k-means и агломеративной кластеризации. Визуализируйте полученные результаты кластеризации с помощью t-SNE и UMAP, выделяя разные кластеры цветом. В этом может помочь аргумент `c` метода `scatter`. Помимо этого, визуализируйте силуэты точек. Вспомогательный код  для этого можно найти в [документации sklearn](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#selecting-the-number-of-clusters-with-silhouette-analysis-on-kmeans-clustering). Хорошо ли работают методы? Попробуйте поизменять  параметры.\n",
    "\n",
    "Мы понимаем, что это не полноценное исследование, а всего лишь домашнее упражнение. Поэтому в выводах не требуем глубокого погружения в предмет и тщательного объяснения происходящего. Достаточно указать какие методы, на ваш взгляд, могут иметь потенциал при изучении данной темы, а какие стоит отбросить на начальном этапе.\n",
    "\n",
    "Оформите всё красиво и структурированно.\n",
    "\n",
    "*) Также можете реализовать на выбор какую-либо метрику качества кластеризации и выводить ее в процессе исследования. Поясните полученные результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yn3mwh2TcaaD"
   },
   "outputs": [],
   "source": [
    "# Стандартизация данных перед кластеризацией\n",
    "X_scaled = StandardScaler().fit_transform(X_pca[:, :optimal_components])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eZzDHc6P7dGv"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "\n",
    "def silhouette_visualize(X, cluster_labels, figsize):\n",
    "    \"\"\"Функция для визуализации силуэтов кластеров.\n",
    "\n",
    "    Args:\n",
    "        X: данные\n",
    "        cluster_labels: метки кластеров\n",
    "        figsize: размер изображения\n",
    "    \"\"\"\n",
    "    silhouette_vals = silhouette_samples(X, cluster_labels)\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    unique_labels = np.unique(cluster_labels)\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    # Для каждого кластера рисуем силуэты\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        if label == -1:\n",
    "            continue  # Пропускаем выбросы, если такие есть\n",
    "\n",
    "        cluster_silhouette_vals = silhouette_vals[cluster_labels == label]\n",
    "        cluster_silhouette_vals.sort()\n",
    "\n",
    "        cluster_size = cluster_silhouette_vals.shape[0]\n",
    "        y_upper = y_lower + cluster_size\n",
    "\n",
    "        plt.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, cluster_silhouette_vals,\n",
    "                          alpha=0.7)\n",
    "\n",
    "        y_lower = y_upper + 10\n",
    "\n",
    "    plt.axvline(silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "    plt.title(\"Silhouette Analysis\")\n",
    "    plt.xlabel(\"Silhouette Coefficient\")\n",
    "    plt.ylabel(\"Cluster\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PSMYVHK57dGv"
   },
   "source": [
    "**DBSCAN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "do-e3UwD7dGv"
   },
   "outputs": [],
   "source": [
    "def DBSCAN_visualize(eps, min_samples):\n",
    "    \"\"\"Функция для визуализации работы DBSCAN\n",
    "\n",
    "    Args:\n",
    "        eps и min_samples: гиперпараметры алгоритма\n",
    "    \"\"\"\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    labels = dbscan.fit_predict(X_scaled)\n",
    "\n",
    "    # Визуализация результатов кластеризации\n",
    "    draw_umap_tsne(figsize=(20, 7), c=labels)\n",
    "\n",
    "    # Выводим силуэтный коэффициент, если кластеры обнаружены\n",
    "    if len(np.unique(labels)) > 1:\n",
    "        silhouette_visualize(X_scaled, labels, figsize=(10, 6))\n",
    "        silhouette_avg = silhouette_score(X_scaled, labels)\n",
    "        print(f\"Silhouette Score for DBSCAN: {silhouette_avg}\")\n",
    "    else:\n",
    "        print(\"No clusters found by DBSCAN.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "AvTk6ewfDZBv",
    "outputId": "dba06025-396b-4fa3-9c21-7e01bb5ecf40"
   },
   "outputs": [],
   "source": [
    "DBSCAN_visualize(eps=2.5, min_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G_t7lk9J7dGv"
   },
   "source": [
    "**Вывод:**\n",
    "\n",
    "**Параметры:** eps = 2.5, min_samples = 5.\n",
    "\n",
    "**Silhouette Score:** 0.4076 — самый высокий среди всех алгоритмов, что указывает на лучшую плотность и однородность кластеров по сравнению с другими методами.\n",
    "\n",
    "**Анализ результатов:** DBSCAN смог разделить данные на несколько плотных кластеров с высоким качеством по силуэтному коэффициенту. Это показывает, что DBSCAN хорошо справляется с задачей, когда данные имеют плотные группы и выбросы.\n",
    "\n",
    "**Заключение:** DBSCAN демонстрирует наилучшие результаты среди всех методов в исходном пространстве данных и, вероятно, будет перспективен для дальнейших исследований. Однако параметр eps нужно точно настраивать для разных данных. А вот это очень муторно... Метод хороший, сам подбирает число кластеров, но вот подбор параметров - очень тонкий процесс."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QxfNYxAh7dGv"
   },
   "source": [
    "**Agglomerative Clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38VoabN87dGw"
   },
   "source": [
    "Для агломеративной кластеризации также постройте дендрограмму. Какого размера получаются кластеры?  Поясните по дендрограмме."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NwBgw0O37dGw"
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W-SgLMBK7dGw"
   },
   "outputs": [],
   "source": [
    "def agglo_visualize(n_clusters):\n",
    "    \"\"\"Функция для визуализации работы агломеративной кластеризаии\n",
    "\n",
    "    Args:\n",
    "        n_clusters: количество кластеров\n",
    "    \"\"\"\n",
    "    agglo = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "    labels = agglo.fit_predict(X_scaled)\n",
    "\n",
    "    draw_umap_tsne(figsize=(20, 7), c=labels)\n",
    "    silhouette_visualize(X_scaled, labels, figsize=(10, 6))\n",
    "    silhouette_avg = silhouette_score(X_scaled, labels)\n",
    "    print(f\"Silhouette Score for Agglomerative Clustering: {silhouette_avg}\")\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    linkage_matrix = linkage(X_scaled, method='ward')\n",
    "    dendrogram(linkage_matrix, truncate_mode='level', p=5)\n",
    "    plt.title(\"Dendrogram for Agglomerative Clustering\")\n",
    "    plt.xlabel(\"Sample Index\")\n",
    "    plt.ylabel(\"Distance\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ij-AuXgbDdqA",
    "outputId": "71d5fb34-0898-4089-ac15-d6ee1781e3a2"
   },
   "outputs": [],
   "source": [
    "agglo_visualize(n_clusters=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IRf8O1G97dGw"
   },
   "source": [
    "**Вывод:**\n",
    "\n",
    "**Параметры:** n_clusters = 14.\n",
    "\n",
    "**Silhouette Score:** 0.0937 — самый низкий среди всех алгоритмов, что свидетельствует о слабой плотности и разделении кластеров.\n",
    "\n",
    "**Анализ результатов:** Иерархическая кластеризация с фиксированным числом кластеров дала более размытые кластеры, что также видно на силуэтной диаграмме. Слабое разделение может быть связано с применением метода Ward, который минимизирует внутрикластерную дисперсию, но не всегда подходит для структур, в которых кластеры имеют разную плотность.\n",
    "\n",
    "**Заключение:** Иерархическая кластеризация может быть менее эффективна на этом наборе данных. Возможно, ее стоит использовать в других представлениях данных или с иными методами связности. Также стоит учесть, то сам алгоритм работает очень долго, поэтому при большом объеме данных ему будет очень плохо."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjj7P6x87dGw"
   },
   "source": [
    "**KMeans**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DKlbIiC67dGw"
   },
   "outputs": [],
   "source": [
    "def kmeans_visualize(n_clusters):\n",
    "    \"\"\"Функция для визуализации работы k-means\n",
    "\n",
    "    Args:\n",
    "        n_clusters: количество кластеров\n",
    "    \"\"\"\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "    draw_umap_tsne(figsize=(20, 7), c=labels)\n",
    "    silhouette_visualize(X_scaled, labels, figsize=(10, 6))\n",
    "    silhouette_avg = silhouette_score(X_scaled, labels)\n",
    "    print(f\"Silhouette Score for KMeans: {silhouette_avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "9Vv3c_-eDe9w",
    "outputId": "573c1978-c9f1-4365-ce82-7562e8552efd"
   },
   "outputs": [],
   "source": [
    "kmeans_visualize(n_clusters=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5fFtCEle7dGw"
   },
   "source": [
    "**Вывод:**\n",
    "\n",
    "**Параметры:** n_clusters = 14.\n",
    "\n",
    "**Silhouette Score:** 0.1782 — выше, чем у иерархической кластеризации, но значительно ниже, чем у DBSCAN.\n",
    "\n",
    "**Анализ результатов:** Метод KMeans смог разбить данные на 14 кластеров с более высокими, но все же умеренными показателями силуэта. Вероятно, это связано с тем, что KMeans лучше подходит для данных с простыми сферическими кластерами и может не справиться с более сложной структурой данных.\n",
    "\n",
    "**Заключение:** KMeans дал приемлемые результаты, но уступает DBSCAN. Он может быть полезен для грубого разбиения данных, но в случае сложных распределений его следует рассматривать осторожно. Но зато он работает быстрее всех."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LnDnDosk7dGw"
   },
   "source": [
    "Сделайте общие выводы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vKvX8-oM7dGw"
   },
   "source": [
    "DBSCAN показал себя лучшим методом для данной задачи, обеспечив максимальный силуэтный коэффициент, что указывает на наибольшую плотность и однородность кластеров. Агломеративная кластеризация и KMeans показали более низкие результаты по силуэту, и хотя KMeans смог создать четкие кластеры, его силуэтный коэффициент остается на среднем уровне. Иерархическая кластеризация показала слабые результаты и потребует дальнейших доработок для повышения качества кластеризации. Но с точки зрения учебной задачи, мы поняли, что параметры нужно настраивать очень долго, для достижения лучшего качества модели."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "stats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
