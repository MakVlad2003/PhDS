## Overview

This repository is a personal â€œcourseâ€‘hubâ€ that tracks my progress through the **PhDS** (ProbabilityÂ &Â DataÂ Science) curriculum.
Inside you will find more than **30 fullyâ€‘executed Jupyter notebooks** that move from classic statistical inference to modern deepâ€‘learning and causalâ€‘inference pipelines:

* Classical tests â€“Â e.g. the twoâ€‘sample **Studentâ€™sÂ *t*â€‘test** for mean comparison([GitHub][1])
* Onlineâ€‘experimentation methodology â€“Â **A/Bâ€‘testing** notebooks with power analysis and uplift charts([Wikipedia][2])
* Core ML algorithms â€“Â **RandomÂ Forests**([Wikipedia][3]) & **GradientÂ Boosting**([PyWhy][4]) for tabular data
* Fundamental deepâ€‘learning â€“Â Multilayer Perceptrons + **ConvolutionalÂ NeuralÂ Networks**Â (CNNs) trained on **CIFARâ€‘10**([Wikipedia][5])
* **Unsupervised learning** â€“Â Kâ€‘Means, hierarchical clustering & dimensionality reduction([Scikit-learn][6])
* **Causal inference** case studies using the openâ€‘source **DoWhy** library([GitHub][7])
* Reproducible code snippets powered by SciPy/NumPy (e.g.Â `scipy.stats.ttest_ind`)([PyWhy][8])

The goal: a single place to revisit theory, runnable code, and accompanying slides/tasks for each topic.

---

## Repository structure

```text
PhDS/
â”œâ”€â”€ HW/                       # Homeâ€‘work notebooks
â”‚   â”œâ”€â”€ 1_Data_Analysis/
â”‚   â”œâ”€â”€ 2_Intro_to_NN/        # feedâ€‘forward & CIFARâ€‘10 CNN experiments
â”‚   â”œâ”€â”€ 3_Intro_to_CNN/
â”‚   â”œâ”€â”€ 4_Segmentation/
â”‚   â”œâ”€â”€ 5_RNN/
â”‚   â”œâ”€â”€ 6_Features_Engineering/
â”‚   â”œâ”€â”€ 7_Random_Forest/
â”‚   â”œâ”€â”€ 8_Gradient_Boosting/
â”‚   â”œâ”€â”€ 9_Unsupervised_Learning/
â”‚   â”œâ”€â”€ 10_t-tests/
â”‚   â”œâ”€â”€ 11_AB-tests_1/
â”‚   â”œâ”€â”€ 12_AB-tests_2/
â”‚   â””â”€â”€ 13_DoWhy/
â”œâ”€â”€ SEM/                      # Seminar notebooks / liveâ€‘coding sessions
â”‚   â””â”€â”€ â€¦ (mirrors most HW topics)
â”œâ”€â”€ .gitattributes            # keeps notebook *outputs* but strips broken widgets
â”œâ”€â”€ .gitignore                # ignores archives & checkpoints
â””â”€â”€ requirements.txt          # minimal runtime deps (optional, see below)
```

> **Why two trees?**
> *HW* directories hold my graded submissions; *SEM* directories hold inâ€‘class walkthroughs and additional explorations.

---

## Quickâ€‘start

1. **Clone** the project

   ```bash
   git clone https://github.com/MakVlad2003/PhDS.git
   cd PhDS
   ```

2. **Create an environment** (conda or venv)

   ```bash
   conda create -n phds python=3.11
   conda activate phds
   pip install -r requirements.txt   # or install packages adâ€‘hoc
   ```

3. **Launch JupyterLab / Notebook**

   ```bash
   jupyter lab
   # or
   jupyter notebook
   ```

4. Open any notebook and **run all cells** â€“Â all heavyâ€‘weight datasets are either
   *downloaded on first run* (e.g. `torchvision.datasets.CIFAR10(download=True)`),
   or stored in lightweight CSV/Pickle form under `data/`.

> **Note on large files**
> Archives (`*.zip`, `*.tar.gz`, â€¦) are Gitâ€‘ignored so the repo stays <Â 100â€¯MB;
> models & figures are regenerated onâ€‘theâ€‘fly when you execute the notebooks.

---

## Folderâ€‘byâ€‘folder tour

| Folder                                  | Highlights                                             | Key concepts                                    |
| --------------------------------------- | ------------------------------------------------------ | ----------------------------------------------- |
| **1\_Data\_Analysis**                   | Pandas EDA, visual storytelling                        | descriptive stats, data cleaning                |
| **2\_Intro\_to\_NN**                    | First fullyâ€‘connected network, weightâ€‘decay experiment | perceptron, SGD                                 |
| **3\_Intro\_to\_CNN / 4\_Segmentation** | CNN on CIFARâ€‘10, Uâ€‘Net miniâ€‘lab                        | kernels, pooling, IoU metric                    |
| **5\_RNN**                              | Characterâ€‘level language model                         | backâ€‘prop through time                          |
| **6\_Features\_Engineering**            | Target / mean encoding, WOEs                           | leakage mitigation                              |
| **7\_Random\_Forest**                   | Gridâ€‘search with OOB score                             | bagging, feature importance                     |
| **8\_Gradient\_Boosting**               | XGBoost & CatBoost headâ€‘toâ€‘head                        | additive models, learning rate                  |
| **9\_Unsupervised\_Learning**           | Kâ€‘Means, Agglomerative, tâ€‘SNE                          | elbow, silhouette                               |
| **10\_t-tests**                         | Oneâ€‘ & twoâ€‘sample tests                                | Studentâ€™s *t*â€‘statistic([GitHub][1])            |
| **11â€‘12\_AB-tests**                     | Power analysis, CUPED uplift                           | A/B experimentation([Wikipedia][2])             |
| **13\_DoWhy**                           | â€œEffect of ads on conversionsâ€ causal graph            | backâ€‘door, propensity score, DoWhy([GitHub][7]) |

---

## Reâ€‘encoding & widget stripping

Broken Jupyterâ€‘widget metadata sometimes prevents GitHub from rendering notebooks.
A custom **`nbsoft`** Git filter (see `.gitattributes`) keeps legitimate *outputs* yet
removes the offending `metadata.widgets` key so notebooks stay viewable online.

If you add new notebooks:

```bash
git add <my_notebook>.ipynb      # the filter runs automatically
git commit -m "Add new lab"
```

---

## Contributing

Pullâ€‘requests are welcome!
Feel free to open an issue for bugs, suggestions or additional exercises.

---

## License

All notebooks & code are released under the **MIT License**.
Datasets retain their original licenses; see individual notebook headers for attributions.

---

### Acknowledgements

*Scikitâ€‘learn, PyTorch, CatBoost, DoWhy and the wider openâ€‘source community* â€“Â made learning & experimentation frictionâ€‘free.
Icons courtesy of the GitHub Octicons set.

---

Enjoy exploring â€“ and happy modelling! ğŸš€

---

<!-- CITATIONS -->

The README references:

* Studentâ€™s *t*â€‘test([GitHub][1])
* CNNs
* RandomÂ Forests([Wikipedia][3])
* GradientÂ Boosting([PyWhy][4])
* UnsupervisedÂ learning([Scikit-learn][6])
* A/BÂ testing([Wikipedia][2])
* CIFARâ€‘10([Wikipedia][5])
* SciPy `ttest_ind`Â API([PyWhy][8])
* DoWhy library([GitHub][7])

[1]: https://github.com/MakVlad2003/PhDS/tree/main/HW "PhDS/HW at main Â· MakVlad2003/PhDS Â· GitHub"
[2]: https://en.wikipedia.org/wiki/A/B_testing "A/B testing - Wikipedia"
[3]: https://en.wikipedia.org/wiki/Student%27s_t-test "Student's t-test - Wikipedia"
[4]: https://pywhy.github.io/dowhy/latest/ "Internal Error"
[5]: https://en.wikipedia.org/wiki/CIFAR-10 "CIFAR-10 - Wikipedia"
[6]: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html "Internal Error"
[7]: https://github.com/py-why/dowhy "GitHub - py-why/dowhy: DoWhy is a Python library for causal inference that supports explicit modeling and testing of causal assumptions. DoWhy is based on a unified language for causal inference, combining causal graphical models and potential outcomes frameworks."
[8]: https://pywhy.github.io/dowhy/latest/getting_started.html "Internal Error"
