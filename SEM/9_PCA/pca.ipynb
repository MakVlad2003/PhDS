{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x54jtW0K1los"
   },
   "source": [
    "# ML в Биологии\n",
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "98c8U_4k1loz"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import accuracy_score as accuracy\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "sns.set_theme(style='white', font_scale=1.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oa_SsmeTJb2H"
   },
   "source": [
    "## Cнижение размерностей\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYUMp-LgJb2J"
   },
   "source": [
    "---\n",
    "### Задача 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srZLT1nSJb2J"
   },
   "source": [
    "В этой задаче мы попробуем применить PCA-разложение для уменьшения размерности данных в задаче классификации. Во многих случаях такая процедура позволяет выделить значимую информацию из датасета при незначительных потерях качества. А в лучшем случае мы избавимся от шумовых признаков и увеличим результат."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJweSzGkJb2K"
   },
   "source": [
    "Будем использовать простой датасет для задачи классификации вин трех разных видов. На практике вам могут встретиться намного более объемные данные, которые имеют большую размерность (представьте, например, что вы закодировали большое число категориальных признаков с помощью one-hot)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WOqqAKVBJb2K"
   },
   "source": [
    "Загрузим данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vmflaHyPJb2K"
   },
   "outputs": [],
   "source": [
    "data = load_wine()\n",
    "X, y = data['data'], data['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0lADZqg4Jb2K"
   },
   "source": [
    "Посмотрим на описание:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GF1amnkeJb2L",
    "outputId": "273297c4-094e-4ad8-b31b-3cb708c4e5d9"
   },
   "outputs": [],
   "source": [
    "print(data['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3NEM4KwmJb2L"
   },
   "source": [
    "Разделим данные на обучающую и тестовую выборки.\n",
    "\n",
    "Для того, чтобы сохранить пропорции каждого класса, используем аргумент `stratify`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tnTwNl9CJb2L"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzadReoiJb2L"
   },
   "source": [
    "Выполните стандартизацию данных. Объясните, почему она необходима в этом случае.\n",
    "\n",
    "**Ответ:** pca работает с метриками, поэтому лучше все стандартизировать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IE_s5ZGVJb2L"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1lL6p6SJb2M"
   },
   "source": [
    "Визуализируйте проекцию выборки на первые две компоненты. Являются ли классы линейно разделимыми?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x1jrprE3KYZB"
   },
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "X_train_comp = pca.fit_transform(X_train_s)\n",
    "X_test_comp = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "YYeb_R0v1lo6",
    "outputId": "642525ac-2448-4b53-f3b0-904967b9b1df"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(x=X_train_comp[:, 0], y=X_train_comp[:, 1], c=y_train, cmap='rainbow')\n",
    "plt.xlabel('Первая главная компонента')\n",
    "plt.ylabel('Вторая главная компонента');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zoHTLsTpJb2M"
   },
   "source": [
    "**Ответ:**\n",
    "\n",
    "Классы данных практически линейно разделимы: лишь 1-2 точки могут оказаться не на своем месте. Однако это утверждение верно только для тренировочной выборки и только в 2D. Если добавить тестовую выборку, линейная разделимость может ухудшиться. В случае 3D или более многомерного пространства данные могут быть разделены гиперплоскостью без особых проблем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ca8lI_zBJb2M"
   },
   "source": [
    "Постройте график зависимости качества логистической регрессии на тестовой выборке от числа компонент. Что можно наблюдать?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 503
    },
    "id": "JOussJK3Jb2M",
    "outputId": "17770551-0407-4491-db3b-22a2fdc7df3a"
   },
   "outputs": [],
   "source": [
    "accs = []\n",
    "\n",
    "for i in range(pca.n_components_):\n",
    "    lr = LogisticRegression(penalty='l2')\n",
    "    lr.fit(X_train[:, :i+1], y_train)\n",
    "    preds = lr.predict(X_test[:, :i+1])\n",
    "    acc = accuracy(y_test, preds)\n",
    "    accs.append(acc)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(accs)\n",
    "\n",
    "plt.xlabel(\"Number of components\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pdURx7yW3S_W",
    "outputId": "3a7dfbe9-f3a2-436e-8cf4-5cd6e427c5cd"
   },
   "outputs": [],
   "source": [
    "print(accs[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I7BTt6-4Jb2M"
   },
   "source": [
    "**Ответ:**\n",
    "\n",
    "Точность повышается с увеличением числа компонентов, достигая своего пика при максимальном количестве. Однако даже наилучший результат в этом случае оказывается хуже, чем простая логистическая регрессия на исходных данных.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54A-zsdwJb2N"
   },
   "source": [
    "Попробуйте обучить обычную логистическую регрессию с Lasso-регуляризацией и выделить самые важные признаки. В чем отличие такого подхода от использования PCA-разложения?\n",
    "\n",
    "*Указание.* Параметр регуляризации достаточно поперебирать вручную."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LMsosb-rJb2N",
    "outputId": "bce75b28-bdb1-41d3-8147-85f1a80c02d2"
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(penalty='l1', solver='saga', multi_class='ovr')\n",
    "lr.fit(X_train_s, y_train)\n",
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RdznOoEk38uS",
    "outputId": "bed3b659-c785-47e5-99d4-ff63dc1111c5"
   },
   "outputs": [],
   "source": [
    "X_train_new = X_train_s\n",
    "X_test_new = X_test_s\n",
    "\n",
    "to_drop = []\n",
    "\n",
    "for i in range(lr.coef_.shape[1]):\n",
    "    if lr.coef_[0][i] == lr.coef_[1][i] == lr.coef_[2][i] == 0:\n",
    "        print(i)\n",
    "        to_drop.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zpkskfuT4BNS",
    "outputId": "96d614af-e63d-419d-b851-a637f100444d"
   },
   "outputs": [],
   "source": [
    "X_train_new = np.delete(X_train_new, to_drop, 1)\n",
    "X_train_s.shape, X_train_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jvSE2Tjg4KCy",
    "outputId": "3a9f2db9-29a9-4512-86e0-bf30c58c9188"
   },
   "outputs": [],
   "source": [
    "X_test_new = np.delete(X_test_new, to_drop, 1)\n",
    "\n",
    "lr1 = LogisticRegression(multi_class='multinomial')\n",
    "lr1.fit(X_train_new, y_train)\n",
    "\n",
    "preds = lr1.predict(X_test_new)\n",
    "acc = accuracy(y_test, preds)\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7c5gcymJb2N"
   },
   "source": [
    "**Ответ:**\n",
    "\n",
    "Результаты оказались аналогичными лучшему сценарию с PCA, но всё равно хуже, чем при использовании обычной логистической регрессии.\n",
    "\n",
    "Вывод: простая логистическая регрессия — лучший вариант."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N7c11JgnJb2N"
   },
   "source": [
    "Какие есть преимущества и недостатки у снижения размерности перед обучением классификатора? Сделайте выводы по задаче."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ERVZDhLUJb2N"
   },
   "source": [
    "**Вывод:**\n",
    "\n",
    "В этом случае простая логистическая регрессия оказалась наилучшей. Это нормально, такое бывает.\n",
    "\n",
    "Однако стоит отметить, что анализ весов логистической регрессии (feature importance) помогает отсеивать шумовые признаки, что может улучшить качество предсказаний. Но этот подход работает только для логрегрессии, тогда как в случае случайного леса важными могут оказаться совсем другие признаки.\n",
    "\n",
    "PCA — более универсальный метод снижения размерности, который можно эффективно использовать на начальном этапе перед применением любой модели. Однако при этом мы неизбежно теряем информацию, что может негативно сказаться на качестве. PCA особенно полезен в задачах, где количество признаков значительно превышает количество образцов.\n",
    "\n",
    "Кроме того, всегда стоит пробовать вариант без сложных методов. Он должен служить базовым уровнем, от которого мы будем отталкиваться."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "stats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
