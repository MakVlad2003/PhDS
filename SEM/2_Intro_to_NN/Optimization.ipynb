{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nb4BcfUNIrpD"
   },
   "source": [
    "# Семинар 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "63xX8XtTJLTf",
    "outputId": "03ca68e1-cc19-44d0-8ada-c34c6ac87359"
   },
   "outputs": [],
   "source": [
    "! apt-get install imagemagick\n",
    "! mkdir saved_gifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bq9w4zVsJPum"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import Image, clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation, ImageMagickFileWriter\n",
    "\n",
    "sns.set(font_scale=1.6, palette='RdBu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uky7o72eI-Hp"
   },
   "source": [
    "## Оптимизаторы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fV1aQmFYOIY2"
   },
   "source": [
    "Вы уже познакомились с основными методами оптимизации, которые широко используются в классическом машинном обучении. С развитием нейронных сетей и активным внедрением нейросетевого подхода, методы оптимизации стали ещё более актуальными. Но стандартные методы оптимизации, SGD и метод тяжёлого шара, имеют ряд недостатков, из-за чего их редко применяют в чистом виде. Для обучения современных нейросетей используют более продвинутые методы.\n",
    "\n",
    "Ключевая особенность всех рассматриваемых ниже методов в том, что они являются адаптивными. Т.е. для различных параметров оптимизируемой функции обновление происходит с различной скоростью.\n",
    "\n",
    "Пусть задача оптимизации имеет вид $f(x) \\longrightarrow \\min\\limits_x$, и $\\nabla_{x} f(x)$ &mdash; градиент функции $f(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iiSc37vROJ1U"
   },
   "source": [
    "\n",
    "Нет универсального метода оптимизации, который всегда работает лучше, чем остальные. Поэтому для выбора наилучшего метода оптимизации и оптимальных гиперпараметров для него проводят ряд экспериментов.\n",
    "Ниже приведена визуализация нескольких экспериментов и сравнение скорости сходимости различных методов оптимизации, запущенных из одной точки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gEV3UAYlOr3S"
   },
   "source": [
    "**1.SGD**\n",
    "\n",
    "Обычный и стохастический градиентный спуск.\n",
    "\n",
    "$$x_{t + 1} = x_t - \\eta v_t,$$\n",
    "\n",
    "где $v_{t} = \\nabla f(x_{t})$ &mdash; аналогия со скоростью."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "vf9-d9hdJR3d",
    "outputId": "46a0b27c-7f98-42f8-aeaf-ff81ab40a1d5"
   },
   "outputs": [],
   "source": [
    "torch.optim.SGD(\n",
    "    [], # Оптимизируемые параметры\n",
    "    lr = 0.01, # Скорость обучения\n",
    "    momentum = 0.0 # уже \"следующий\" метод\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "McP_YmJcO-TB"
   },
   "source": [
    "**2. SGD + Momentum**\n",
    "\n",
    "Сгладим градиент, используя информацию о том, как градиент изменялся раньше.\n",
    "Физическая аналогия &mdash; добавляем инерцию.\n",
    "\n",
    "\n",
    "$$x_{t + 1} = x_t + v_{t},$$\n",
    "где $v_{t} = \\mu v_{t - 1} - \\eta \\nabla f(x_{t})$ &mdash; сглаживаем градиенты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "kyovMXcaPGGX",
    "outputId": "73d4187d-7f9f-4529-aa2c-07c1851670ec"
   },
   "outputs": [],
   "source": [
    "torch.optim.SGD(\n",
    "    [], # Оптимизируемые параметры\n",
    "    lr = 0.01, # Скорость обучения\n",
    "    momentum = 0.9 # Инертность модели, \"масса\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QA4iQdxNPGtr"
   },
   "source": [
    "**3. Adagrad**\n",
    "\n",
    "Adagrad &mdash; один из самых первых адаптивных методов оптимизации.\n",
    "\n",
    "Во всех изученных ранее методах есть необходимость подбирать шаг метода (коэффициент $\\eta$). На каждой итерации все компоненты градиента оптимизируемой функции домножаются на одно и то же число $\\eta$. Но использовать одно значение $\\eta$ для всех параметров не оптимально, так как они имеют различные распределения и оптимизируемая функция изменяется с совершенно разной скоростью при небольших изменениях разных параметров.\n",
    "\n",
    "Поэтому гораздо логичнее **изменять значение каждого параметра с индивидуальной скоростью**. При этом, *чем c большей степени от изменения параметра меняется значение оптимизируемой функции, тем с меньшей скоростью стоить обновлять этот параметр*. Иначе высок шанс расходимости метода. Получить такой результат удается, если разделить градиент на сумму квадратов скорости изменений параметров.\n",
    "\n",
    "Пусть $x^{(i)}$ &mdash; $i$-я компонента вектора $x$.\n",
    "$$x_{t+1, i} = x_{t, i} - \\frac{\\eta}{\\sqrt{g_{t, i}+\\varepsilon}}\\cdot \\nabla f_i(x_t)$$\n",
    "$$g_{t} = g_{t-1} + \\nabla f(x_t) \\odot \\nabla f(x_t)$$\n",
    "\n",
    "\n",
    "В матрично-векторном виде шаг алгоритма можно переписать так:\n",
    "$$x_{t+1} = x_{t} - \\frac{\\eta}{\\sqrt{g_{t} + \\varepsilon}}\\odot \\nabla f(x_t).$$\n",
    "Здесь $\\odot$ обозначает произведение Адамара, т.е. поэлементное перемножение векторов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "tnCu4D4WPOqm",
    "outputId": "df822883-0d39-4998-dd1b-4a1e66c82255"
   },
   "outputs": [],
   "source": [
    "torch.optim.Adagrad(\n",
    "    [], # оптимизируемые параметры\n",
    "    lr = 0.01, # скорость обучения\n",
    "    eps = 1e-10 # epsilon задана дефолтно\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9bH6so6lPPEr"
   },
   "source": [
    "**4. RMSProp**\n",
    "\n",
    "Алгоритм RMSProp основан на той же идее, что и алгоритм Adagrad &mdash; адаптировать learning rate отдельно для каждого параметра $\\theta^{(i)}$.  Однако Adagrad имеет серьёзный недостаток. Он с одинаковым весом учитывает квадраты градиентов как с самых первых итераций, так и с самых последних. Хотя, на самом деле, наибольшую значимость имеют модули градиентов на последних нескольких итерациях.\n",
    "\n",
    "Для этого предлагается использовать **экспоненциальное сглаживание**.\n",
    "$$x_{t+1} = x_{t} - \\frac{\\eta}{\\sqrt{g_{t} + \\varepsilon}}\\odot \\nabla f(x_t).$$\n",
    "$$g_t = \\mu g_{t-1} + (1-\\mu) \\nabla f(x_t) \\odot \\nabla f(x_t)$$\n",
    "\n",
    "Стандартные значения гиперпараметров: $\\mu = 0.9, \\eta = 0.001$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "NBAHAwbHPQQp",
    "outputId": "6ad4554a-c519-4455-e6f0-a19696105d85"
   },
   "outputs": [],
   "source": [
    "torch.optim.RMSprop(\n",
    "    [], # Оптимизируемые параметры\n",
    "    lr = 0.01, # Eta в обозначениях выше, дефолтное значение скорости обучения\n",
    "    alpha = 0.99, # Mu в обозначении выше, дефолтное значение параметра\n",
    "    eps = 1e-08 # Epsilon задана дефолтно\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_rG8H7XdTAzh"
   },
   "source": [
    "**5. Adadelta**\n",
    "\n",
    "Этот метод по формуле шага и по смыслу очень похож на RMSProp. Авторы метода заметили, что в различных методах 1 порядка для оптимальной сходимости нужно брать совершенно разные значения learning rate ($\\eta$), а иногда − подбирать значение $\\eta$ в зависимости от решаемой задачи. Чтобы избавиться от необходимости находить наилучшее значение $\\eta$. Для этого корень среднеквадратичной ошибки обновления параметра (RMS) считается теперь и для $\\Delta \\theta$.\n",
    "$$d_t = \\mu d_{t-1} + (1-\\mu)\\Delta x_t \\odot \\Delta x_t$$\n",
    "$$\\Delta x_{t} = -\\frac{\\sqrt{d_{t-1}+\\varepsilon}}{\\sqrt{g_{t} + \\varepsilon}}\\odot \\nabla Q(x_t)$$\n",
    "$$x_{t+1} = x_t + \\Delta x_t$$\n",
    "\n",
    "Преимущество данного метода по сравнению с RMSProp − отсутствие необходимости подбирать значения параметра $\\eta$.\n",
    "Экспериментальным путём выяснено, что для Adadelta наилучшее значение $\\mu \\sim 0.9$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "67c_9AeyTAkc"
   },
   "outputs": [],
   "source": [
    "torch.optim.Adadelta(\n",
    "    [], # Оптимизируемые параметры\n",
    "    lr = 1.0, # Параметр скорости обучения\n",
    "    rho = 0.9, # Mu в наших обозначениях, дефолтное значение\n",
    "    eps = 1e-06 # Epsilon, дефолтное значение\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h66O57baPe_D"
   },
   "source": [
    "**6. Adam**\n",
    "\n",
    "Этот алгоритм совмещает в себе 2 идеи:\n",
    "* идею алгоритма Momentum *о накапливании градиента*,\n",
    "* идею методов Adadelta и RMSProp *об экспоненциальном сглаживании* информации о предыдущих значениях квадратов градиентов.\n",
    "\n",
    "Благодаря использованию этих двух идей, метод имеет 2 преимущества над большей частью методов первого порядка, описанных выше:\n",
    "\n",
    "\n",
    "1. Он обновляет все параметры $\\theta$ не с одинаковым `learning rate`, а выбирает для каждого $\\theta_i$ индивидуальный `learning rate`, что *позволяет учитывать разреженные признаки с большим весом*.\n",
    "\n",
    "\n",
    "2. Adam за счёт применения экспоненциального сглаживания к градиенту *работает более устойчиво в окрестности оптимального значения параметра $\\theta^*$*, чем методы, использующие градиент в точке $x_t$, не накапливая значения градиента с прошлых шагов.\n",
    "\n",
    "\n",
    "Формулы шага алгоритма выглядят так:\n",
    "$$v_{t + 1} = \\beta v_{t} + (1-\\beta) \\nabla x(x_{t})$$\n",
    "$$g_t = \\mu g_{t-1} + (1-\\mu) \\nabla x(x_t) \\odot \\nabla x(x_t)$$\n",
    "\n",
    "Чтобы эти оценки не были смещёнными, нужно их отнормировать:\n",
    "$$\\widehat{v}_{t + 1} = \\frac{v_{t + 1}}{1-\\beta^t},$$\n",
    "$$\\widehat{g}_t = \\frac{g_t}{1-\\mu^t}.$$\n",
    "\n",
    "Тогда получим итоговую формулу шага:\n",
    "\n",
    "$$x_{t+1} = x_t - \\frac{\\eta}{\\sqrt{\\widehat{g}_t + \\varepsilon}} \\odot \\widehat{v}_{t + 1}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eazPExPsPx_V"
   },
   "outputs": [],
   "source": [
    "torch.optim.Adam(\n",
    "    [], # оптимизируемые параметры\n",
    "    lr = 0.001, # Eta в наших обозначениях, скорость обучения\n",
    "    betas = (0.9, 0.999), # Параметры mu и beta в наших обозначениях\n",
    "    eps = 1e-08 # Epsilon в наших обозначениях\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF-FfdmePzYt"
   },
   "source": [
    "**Статьи**\n",
    "\n",
    "Для того, чтобы подробнее познакомиться с представленными выше методами, мотивацией их авторов и теоретическими оценками сходимости, можно прочитать оригинальные статьи.\n",
    "\n",
    "* Adagrad: http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf,\n",
    "\n",
    "* Adadelta: https://arxiv.org/abs/1212.5701,\n",
    "\n",
    "* Adam: https://arxiv.org/abs/1412.6980.\n",
    "\n",
    "Как можно заметить, для нейросетей мы рассматриваем только методы оптимизации первого порядка. Это связано с тем, что эффективные архитектуры нейронных сетей имеют большое количество параметров, из-за чего методы второго порядка, требующие на одну итерацию $O(d^2)$ памяти и выполняющие $O(d^3)$ операций, работают слишком долго и их преимущество в количестве итераций до сходимости утрачивает смысл."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IcE2uFzDbgPQ"
   },
   "source": [
    "### Эксперименты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93UyOgZ3bWyv"
   },
   "source": [
    "Вспомогательные функции, они нам понадобятся в дальнейшем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5bwImWIbbO6N"
   },
   "outputs": [],
   "source": [
    "def make_experiment(func, trajectory, graph_title,\n",
    "                    min_y=-7, max_y=7, min_x=-7, max_x=7):\n",
    "    '''\n",
    "    Функция, которая для заданной функции рисует её линии уровня,\n",
    "    а также траекторию сходимости метода оптимизации.\n",
    "\n",
    "    Параметры.\n",
    "    1) func - оптимизируемая функция,\n",
    "    2) trajectory - траектория метода оптимизации,\n",
    "    3) graph_name - заголовок графика.\n",
    "    '''\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    xdata, ydata = [], []\n",
    "    ln, = plt.plot([], [])\n",
    "\n",
    "    mesh_x = np.linspace(min_x, max_x, 300)\n",
    "    mesh_y = np.linspace(min_y, max_y, 300)\n",
    "    X, Y = np.meshgrid(mesh_x, mesh_y)\n",
    "    Z = np.zeros((len(mesh_x), len(mesh_y)))\n",
    "\n",
    "    for coord_x in range(len(mesh_x)):\n",
    "        for coord_y in range(len(mesh_y)):\n",
    "            Z[coord_y, coord_x] = func(\n",
    "                torch.tensor((mesh_x[coord_x],\n",
    "                          mesh_y[coord_y]))\n",
    "            )\n",
    "\n",
    "    def init():\n",
    "        ax.contour(\n",
    "            X, Y, np.log(Z),\n",
    "            np.log([0.5, 10, 30, 80, 130, 200, 300, 500, 900]),\n",
    "            cmap='winter'\n",
    "        )\n",
    "        ax.set_title(graph_title)\n",
    "        return ln,\n",
    "\n",
    "    def update(frame):\n",
    "        xdata.append(trajectory[frame][0])\n",
    "        ydata.append(trajectory[frame][1])\n",
    "        ln.set_data(xdata, ydata)\n",
    "        return ln,\n",
    "\n",
    "    ani = FuncAnimation(\n",
    "        fig, update, frames=range(len(trajectory)),\n",
    "        init_func=init, repeat=True\n",
    "    )\n",
    "    writer = ImageMagickFileWriter(fps=10)\n",
    "    ani.save(f'saved_gifs/{graph_title}.gif',\n",
    "             writer=writer)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wrcTp8TobPGs"
   },
   "outputs": [],
   "source": [
    "def optimize(optimizer, func_optimize, parameters, n_iter):\n",
    "    \"\"\"\n",
    "    Функция, которая для заданных функции и оптимизатора изменяет параметры,\n",
    "    сохраняя их в качестве истории вместе со значениями функциями, которая\n",
    "    оптимизируется.\n",
    "\n",
    "    Параметры.\n",
    "    1) optimizer - оптимизатор\n",
    "    2) func_optimize - оптимизируемая функция\n",
    "    3) parameters - оптимизируемые параметры\n",
    "    4) n_iter - число итераций алгоритма\n",
    "    \"\"\"\n",
    "    history = [parameters.detach().clone().numpy()]\n",
    "    losses = []\n",
    "    for _ in range(n_iter):\n",
    "      loss = func_optimize(parameters)\n",
    "      losses.append(loss.detach().clone().numpy())\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      history.append(parameters.detach().clone().numpy())\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(losses)\n",
    "    plt.title(\"Зависимость значения функции от шага оптимизации\")\n",
    "    plt.xlabel(\"Номер шага\")\n",
    "    plt.ylabel(\"Значение функции\")\n",
    "    return history, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJyS2GPicOGf"
   },
   "source": [
    "**SGD**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PIhNCIb_dB2X"
   },
   "source": [
    "Зададим параметры, оптимизатор, оптимизируемую функцию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uq3bNvWWcH08"
   },
   "outputs": [],
   "source": [
    "x_init = torch.tensor([7., -7.])\n",
    "x = x_init.clone()\n",
    "x.requires_grad = True\n",
    "optimizer = torch.optim.SGD([x], lr=0.01, momentum=0.0)\n",
    "\n",
    "def func_optimize(x):\n",
    "    return 10 * x[0] ** 2 + x[1] **2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1KmB6oWdJZh"
   },
   "source": [
    "Минимизируем нашу функцию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 596
    },
    "id": "jduytzO8dAym",
    "outputId": "99c28423-55a0-4e23-e697-7b8fe9407342"
   },
   "outputs": [],
   "source": [
    "history, losses = optimize(optimizer, func_optimize, x, n_iter=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cj_H5FaadY9Q"
   },
   "source": [
    "Посмотрим, как оно обучалось."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817
    },
    "id": "PD4sfJUYdZR7",
    "outputId": "350a442c-3e95-4d98-8f2d-08800beed198"
   },
   "outputs": [],
   "source": [
    "graph_title = 'simple_test'\n",
    "make_experiment(func_optimize, history, graph_title=graph_title,\n",
    "                    min_y=-7, max_y=7, min_x=-7, max_x=7)\n",
    "clear_output()\n",
    "Image(open(f'saved_gifs/{graph_title}.gif','rb').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6CNQ_4-2kqBv"
   },
   "source": [
    "Более сложный пример"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yAfswTnpkqYp"
   },
   "outputs": [],
   "source": [
    "x_init = torch.tensor([7., -7.])\n",
    "x = x_init.clone()\n",
    "x.requires_grad = True\n",
    "optimizer = torch.optim.SGD([x], lr=0.01, momentum=0.0)\n",
    "\n",
    "def func_optimize(x):\n",
    "    return (x[0]**2+x[1]**2)*(torch.sin(3*x[0])*torch.cos(2*x[1])+x[0]**2/14+x[1]**2/14) - (x[0]+x[1]+x[0]*x[1]/7)**2+80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 596
    },
    "id": "-hy0BYrWkq5v",
    "outputId": "769a4ec1-cc04-44ac-b208-81bd10a0cfa8"
   },
   "outputs": [],
   "source": [
    "history, losses = optimize(optimizer, func_optimize, x, n_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817
    },
    "id": "SKjTYhEvkrEb",
    "outputId": "9c341356-2340-42fd-b721-d3659c3f71ad"
   },
   "outputs": [],
   "source": [
    "graph_title = 'other_test'\n",
    "make_experiment(func_optimize, history, graph_title=graph_title,\n",
    "                    min_y=-7, max_y=7, min_x=-7, max_x=7)\n",
    "clear_output()\n",
    "Image(open(f'saved_gifs/{graph_title}.gif','rb').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "agSYdDfbqX4A"
   },
   "source": [
    "**Задание 1**\n",
    "\n",
    "По аналогии с написанным выше проведите эксперименты с остальными оптимизаторами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zr3kHArndoQc"
   },
   "source": [
    "**SGD + weight**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_OpwXu1yqiin"
   },
   "source": [
    "Простой пример"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OLrrVtkwduN5"
   },
   "outputs": [],
   "source": [
    "x_init = torch.tensor([7., -7.])\n",
    "x = x_init.clone()\n",
    "x.requires_grad = True\n",
    "optimizer = torch.optim.SGD([x], lr=0.01, momentum=0.9)\n",
    "\n",
    "def func_optimize(x):\n",
    "    return 10 * x[0] ** 2 + x[1] **2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 596
    },
    "id": "0jRO1uYZ9Aln",
    "outputId": "537dcc72-b0e1-4dbb-e679-604f18dfa42e"
   },
   "outputs": [],
   "source": [
    "history, losses = optimize(optimizer, func_optimize, x, n_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817
    },
    "id": "ApiLuHRB9Au6",
    "outputId": "6958cc0a-ed37-4b5c-94ca-f66cad10430f"
   },
   "outputs": [],
   "source": [
    "graph_title = 'simple_test'\n",
    "make_experiment(func_optimize, history, graph_title=graph_title,\n",
    "                    min_y=-7, max_y=7, min_x=-7, max_x=7)\n",
    "clear_output()\n",
    "Image(open(f'saved_gifs/{graph_title}.gif','rb').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YrI7q2J5qgC6"
   },
   "source": [
    "Сложный пример"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_eSc6cPpqgL7"
   },
   "outputs": [],
   "source": [
    "x_init = torch.tensor([7., -7.])\n",
    "x = x_init.clone()\n",
    "x.requires_grad = True\n",
    "optimizer = torch.optim.SGD([x], lr=0.01, momentum=0.9)\n",
    "\n",
    "def func_optimize(x):\n",
    "    return (x[0]**2+x[1]**2)*(torch.sin(3*x[0])*torch.cos(2*x[1])+x[0]**2/14+x[1]**2/14) - (x[0]+x[1]+x[0]*x[1]/7)**2+80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 596
    },
    "id": "zvK_mBxi9H9y",
    "outputId": "52c26001-3297-4fa1-da23-9d2f096032a3"
   },
   "outputs": [],
   "source": [
    "history, losses = optimize(optimizer, func_optimize, x, n_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817
    },
    "id": "rWlXtdrT9IFN",
    "outputId": "726337e9-d485-469e-f804-246abfd6dcaa"
   },
   "outputs": [],
   "source": [
    "graph_title = 'other_test'\n",
    "make_experiment(func_optimize, history, graph_title=graph_title,\n",
    "                    min_y=-7, max_y=7, min_x=-7, max_x=7)\n",
    "clear_output()\n",
    "Image(open(f'saved_gifs/{graph_title}.gif','rb').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9L-V2dwGigw4"
   },
   "source": [
    "**Adagrad**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YopXcrxzq7QB"
   },
   "source": [
    "Простой пример"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s7HciRYDq7QB"
   },
   "outputs": [],
   "source": [
    "x_init = torch.tensor([7., -7.])\n",
    "x = x_init.clone()\n",
    "x.requires_grad = True\n",
    "optimizer = torch.optim.Adagrad([x], lr=0.8, eps = 1e-10)\n",
    "\n",
    "def func_optimize(x):\n",
    "    return 10 * x[0] ** 2 + x[1] **2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 596
    },
    "id": "fGDoHcTo9rXp",
    "outputId": "c960614a-6c41-4e8a-b761-d59f9b2ec1ac"
   },
   "outputs": [],
   "source": [
    "history, losses = optimize(optimizer, func_optimize, x, n_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817
    },
    "id": "J5xAqlBD9rfC",
    "outputId": "dd4c2024-dfc7-4d63-cbe2-32d27ea91d2d"
   },
   "outputs": [],
   "source": [
    "graph_title = 'simple_test'\n",
    "make_experiment(func_optimize, history, graph_title=graph_title,\n",
    "                    min_y=-7, max_y=7, min_x=-7, max_x=7)\n",
    "clear_output()\n",
    "Image(open(f'saved_gifs/{graph_title}.gif','rb').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bkJPFnoOq7QB"
   },
   "source": [
    "Сложный пример"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Woq_y2V9q7QC"
   },
   "outputs": [],
   "source": [
    "x_init = torch.tensor([7., -7.])\n",
    "x = x_init.clone()\n",
    "x.requires_grad = True\n",
    "optimizer = torch.optim.Adagrad([x], lr=0.8, eps = 1e-10)\n",
    "\n",
    "def func_optimize(x):\n",
    "    return (x[0]**2+x[1]**2)*(torch.sin(3*x[0])*torch.cos(2*x[1])+x[0]**2/14+x[1]**2/14) - (x[0]+x[1]+x[0]*x[1]/7)**2+80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 596
    },
    "id": "9hL5g6TR9so3",
    "outputId": "e23e1346-c724-4a8c-c858-4c7053202407"
   },
   "outputs": [],
   "source": [
    "history, losses = optimize(optimizer, func_optimize, x, n_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817
    },
    "id": "r3KcLvxh9sxN",
    "outputId": "15930a1c-96e9-44b0-ca5c-52e42e3838cd"
   },
   "outputs": [],
   "source": [
    "graph_title = 'other_test'\n",
    "make_experiment(func_optimize, history, graph_title=graph_title,\n",
    "                    min_y=-7, max_y=7, min_x=-7, max_x=7)\n",
    "clear_output()\n",
    "Image(open(f'saved_gifs/{graph_title}.gif','rb').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uukZcseripsk"
   },
   "source": [
    "**RMSProp**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wreZdC3_rAdn"
   },
   "source": [
    "Простой пример"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WHhTkOOorAdn"
   },
   "outputs": [],
   "source": [
    "x_init = torch.tensor([7., -7.])\n",
    "x = x_init.clone()\n",
    "x.requires_grad = True\n",
    "optimizer = torch.optim.RMSprop([x], lr=0.1, alpha = 0.99, eps = 1e-08)\n",
    "\n",
    "def func_optimize(x):\n",
    "    return 10 * x[0] ** 2 + x[1] **2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 596
    },
    "id": "c2gc-s4X9x3s",
    "outputId": "d1831ccf-add4-4da1-af36-96aea7961185"
   },
   "outputs": [],
   "source": [
    "history, losses = optimize(optimizer, func_optimize, x, n_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817
    },
    "id": "I0_hoblq9yAt",
    "outputId": "d88faf1d-540c-426a-d9ad-5c866834c513"
   },
   "outputs": [],
   "source": [
    "graph_title = 'simple_test'\n",
    "make_experiment(func_optimize, history, graph_title=graph_title,\n",
    "                    min_y=-7, max_y=7, min_x=-7, max_x=7)\n",
    "clear_output()\n",
    "Image(open(f'saved_gifs/{graph_title}.gif','rb').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yfgUTVtfrAdn"
   },
   "source": [
    "Сложный пример"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZPfkHugerAdn"
   },
   "outputs": [],
   "source": [
    "x_init = torch.tensor([7., -7.])\n",
    "x = x_init.clone()\n",
    "x.requires_grad = True\n",
    "optimizer = torch.optim.RMSprop([x], lr=0.1, alpha = 0.99, eps = 1e-08)\n",
    "\n",
    "def func_optimize(x):\n",
    "    return (x[0]**2+x[1]**2)*(torch.sin(3*x[0])*torch.cos(2*x[1])+x[0]**2/14+x[1]**2/14) - (x[0]+x[1]+x[0]*x[1]/7)**2+80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 596
    },
    "id": "CJ2NVv5e9zON",
    "outputId": "ada99a5d-98c2-457d-880f-947351e43a02"
   },
   "outputs": [],
   "source": [
    "history, losses = optimize(optimizer, func_optimize, x, n_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817
    },
    "id": "h8qK_PZI9zVI",
    "outputId": "c436b193-3526-43b5-907b-d15c0065be14"
   },
   "outputs": [],
   "source": [
    "graph_title = 'other_test'\n",
    "make_experiment(func_optimize, history, graph_title=graph_title,\n",
    "                    min_y=-7, max_y=7, min_x=-7, max_x=7)\n",
    "clear_output()\n",
    "Image(open(f'saved_gifs/{graph_title}.gif','rb').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nTVuC7lqiqlu"
   },
   "source": [
    "**Adadelta**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIkewkbNrCos"
   },
   "source": [
    "Простой пример"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sv4AMwKsrCos"
   },
   "outputs": [],
   "source": [
    "x_init = torch.tensor([7., -7.])\n",
    "x = x_init.clone()\n",
    "x.requires_grad = True\n",
    "optimizer = torch.optim.Adadelta([x], lr=3.0, rho = 0.9, eps = 1e-06)\n",
    "\n",
    "def func_optimize(x):\n",
    "    return 10 * x[0] ** 2 + x[1] **2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 596
    },
    "id": "jzPiGnIe90bi",
    "outputId": "0b9aa74a-ce20-4c68-defb-1a4f1deee3a0"
   },
   "outputs": [],
   "source": [
    "history, losses = optimize(optimizer, func_optimize, x, n_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817
    },
    "id": "WF0nM5G090jR",
    "outputId": "12bdd8f8-e5b2-49a6-a5db-384e43671e07"
   },
   "outputs": [],
   "source": [
    "graph_title = 'simple_test'\n",
    "make_experiment(func_optimize, history, graph_title=graph_title,\n",
    "                    min_y=-7, max_y=7, min_x=-7, max_x=7)\n",
    "clear_output()\n",
    "Image(open(f'saved_gifs/{graph_title}.gif','rb').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wx4u-JC5rCos"
   },
   "source": [
    "Сложный пример"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vUP9TTJBrCos"
   },
   "outputs": [],
   "source": [
    "x_init = torch.tensor([7., -7.])\n",
    "x = x_init.clone()\n",
    "x.requires_grad = True\n",
    "optimizer = torch.optim.Adadelta([x], lr=3.0, rho = 0.9, eps = 1e-06)\n",
    "\n",
    "def func_optimize(x):\n",
    "    return (x[0]**2+x[1]**2)*(torch.sin(3*x[0])*torch.cos(2*x[1])+x[0]**2/14+x[1]**2/14) - (x[0]+x[1]+x[0]*x[1]/7)**2+80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 596
    },
    "id": "U2KACxFl91oH",
    "outputId": "f677780b-8265-4720-f2db-38f45cda86c1"
   },
   "outputs": [],
   "source": [
    "history, losses = optimize(optimizer, func_optimize, x, n_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817
    },
    "id": "kBsBL-ei91w5",
    "outputId": "daac0c89-8e63-434e-f489-91f953808ba2"
   },
   "outputs": [],
   "source": [
    "graph_title = 'other_test'\n",
    "make_experiment(func_optimize, history, graph_title=graph_title,\n",
    "                    min_y=-7, max_y=7, min_x=-7, max_x=7)\n",
    "clear_output()\n",
    "Image(open(f'saved_gifs/{graph_title}.gif','rb').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3P0A10ai3kI"
   },
   "source": [
    "**Adam**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mvoI3bYdrD9E"
   },
   "source": [
    "Простой пример"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w-BP9WRPrD9E"
   },
   "outputs": [],
   "source": [
    "x_init = torch.tensor([7., -7.])\n",
    "x = x_init.clone()\n",
    "x.requires_grad = True\n",
    "optimizer = torch.optim.Adam([x], lr=0.1, betas = (0.9, 0.999), eps = 1e-08)\n",
    "\n",
    "def func_optimize(x):\n",
    "    return 10 * x[0] ** 2 + x[1] **2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 596
    },
    "id": "TrrnwVyW928A",
    "outputId": "de7cabb1-c271-4b3a-c8ec-d5e3b811c1e4"
   },
   "outputs": [],
   "source": [
    "history, losses = optimize(optimizer, func_optimize, x, n_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817
    },
    "id": "G9JKCMeC93FL",
    "outputId": "42d49b92-9b71-4991-d106-1242cea3ffcf"
   },
   "outputs": [],
   "source": [
    "graph_title = 'simple_test'\n",
    "make_experiment(func_optimize, history, graph_title=graph_title,\n",
    "                    min_y=-7, max_y=7, min_x=-7, max_x=7)\n",
    "clear_output()\n",
    "Image(open(f'saved_gifs/{graph_title}.gif','rb').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LA5LSjqBrD9E"
   },
   "source": [
    "Сложный пример"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IDeVZBDtrD9E"
   },
   "outputs": [],
   "source": [
    "x_init = torch.tensor([7., -7.])\n",
    "x = x_init.clone()\n",
    "x.requires_grad = True\n",
    "optimizer = torch.optim.Adam([x], lr=0.1, betas = (0.9, 0.999), eps = 1e-08)\n",
    "\n",
    "def func_optimize(x):\n",
    "    return (x[0]**2+x[1]**2)*(torch.sin(3*x[0])*torch.cos(2*x[1])+x[0]**2/14+x[1]**2/14) - (x[0]+x[1]+x[0]*x[1]/7)**2+80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 596
    },
    "id": "-eg7ZCuW93_X",
    "outputId": "b8bd0c72-ff94-4c47-819d-5fb0f49017bd"
   },
   "outputs": [],
   "source": [
    "history, losses = optimize(optimizer, func_optimize, x, n_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817
    },
    "id": "BKokzBWY94Hb",
    "outputId": "d409fc50-0ff0-4db3-9517-2494d95b9830"
   },
   "outputs": [],
   "source": [
    "graph_title = 'other_test'\n",
    "make_experiment(func_optimize, history, graph_title=graph_title,\n",
    "                    min_y=-7, max_y=7, min_x=-7, max_x=7)\n",
    "clear_output()\n",
    "Image(open(f'saved_gifs/{graph_title}.gif','rb').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qiFZ7GI-i61h"
   },
   "source": [
    "**Выводы:**\n",
    "\n",
    "Наверное, самое правильное, что можно сказать касательно этого задания: подбор гиперпараметров - это целое искусство. Будь времени побольше, можно бы было подобрать получить результаты лучше.\n",
    "\n",
    "Также стоит обратить внимание на поведение функций, т.е. как они сходятся к минимумы. Мы можем выидеть и линейные и хаотичные."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
