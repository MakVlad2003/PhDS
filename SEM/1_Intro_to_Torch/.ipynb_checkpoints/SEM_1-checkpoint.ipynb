{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JEFYtxdXQP1s"
   },
   "source": [
    "# <a href=\"https://miptstats.github.io/courses/ad_mipt.html\">Phystech@DataScience</a>\n",
    "\n",
    "\n",
    "## PyTorch и полносвязные нейронные сети\n",
    "\n",
    "![pytorch-logo.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmmsgPQiH7A9"
   },
   "source": [
    "Обучение нейронной сети заключается в минимизации некоторой функции потерь, зависящей от значений активации нейронов. Так как мы знаем весь процесс построения этой функции, то для минимизации нам достоточно посчитать градиент от всех параметров функции (нейронов) и затем двигаться по нему, обновляя значения параметров. Однако современные нейронные сети могут содержать в себе миллиарды параметров (например, GPT-3 содержит 175 миллиардов, а гугловская PaLM 540 миллиардов!), что делает вычисление градиента вручную невозможным. К счастью, люди придумали как автоматизировать этот процесс с помощью вычислительного графа, и написали библиотеки (Keras, TensorFlow, PyTorch), которые занимаются этим за нас :)\n",
    "\n",
    "PyTorch - это один из фреймфорков для задач глубокого обучения, то есть библиотека, позволяющая:\n",
    "* Производить быстрые матричные вычисления с использованием видеопамяти\n",
    "* Строить вычислителный граф и дифференцировать его (то есть сможем получать градиент функции потерь)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wp5-n1308hjM",
    "outputId": "f5d90a7d-46e1-4357-ddb2-496e80d8aea2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "garaktUdSXDG"
   },
   "source": [
    "# Torch Tensor\n",
    "\n",
    "Основным объектом, с которым оперирует PyTorch, является `torch.tensor`. Он представляет собой многомерную матрицу с элементами одного типа данных. Работа с тензорами в PyTorch похожа на работу с массивами в `numpy`. Посмотрим на некоторые методы `torch.tensor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jv9tQc0OVTTp"
   },
   "source": [
    "### Инициализация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wBIVi72lTT0O"
   },
   "source": [
    "Тензор можно создать из списка, кортежа, массива numpy или просто числа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PPnSgi7ATTRA",
    "outputId": "0af66e80-9949-454a-ec56-5b42443db6db"
   },
   "outputs": [],
   "source": [
    "print(torch.tensor([1, 2, 3]), 'тензор из списка')\n",
    "print(torch.tensor((1, 2, 3)), 'тензор из кортежа')\n",
    "print(torch.tensor(np.array([1, 2, 3])), 'тензор из массива numpy')\n",
    "print(torch.tensor((1)), 'тензор из числа')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XXlfzxAgVJte"
   },
   "source": [
    "Так же, как и в `numpy`, тензор может быть создан произвольного размера. Узнать размер тензора можно вызвав метод `size()` или посмотрев атрибут `shape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DWVEeHiZVE-b",
    "outputId": "385d4bdf-bb86-4781-fc5b-0d972a8082b1"
   },
   "outputs": [],
   "source": [
    "t = torch.tensor([[1, 2, 3], [2, 3, 4]])\n",
    "\n",
    "print(t)\n",
    "print(t.size(), 'метод `size()`')\n",
    "print(t.shape, 'атрибут shape')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EGhWo3cscy0h"
   },
   "source": [
    "Можно конкретизировать тип данных с помощью аргумент `dtype`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "62wv1czpc6dO",
    "outputId": "17f77e6f-a78b-409b-88ca-61719866b2ae"
   },
   "outputs": [],
   "source": [
    "t = torch.tensor([[1, 2, 3], [2, 3, 4]], dtype=torch.float64)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UpKUyGWvWiOE"
   },
   "source": [
    "Аналогично `numpy` есть методы создания матриц, заполненных нулями и единицами: `zeros()`, `ones()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Tsk557SXF2Q"
   },
   "source": [
    "### Математические операции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZePxuu5W9jA"
   },
   "source": [
    "Над тензорами можно производить математические операции, так же как и в `numpy`. Например складывать с числом и умножать на число"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1c624VRaXEiE",
    "outputId": "29e08c80-e892-480e-a7c1-61e12f3d8af3"
   },
   "outputs": [],
   "source": [
    "a = torch.ones((2, 3))\n",
    "\n",
    "print(a, 'изначальный тензор A\\n')\n",
    "\n",
    "print(a + 1, 'A + 1\\n')\n",
    "\n",
    "print(a * 3, 'A * 3\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0q1O4T1rYQ74"
   },
   "source": [
    "Складывать и умножать два тезора (поэлементно)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qOkP3vkVYC9n",
    "outputId": "0689500f-68ee-4891-c599-4082b51cdda5"
   },
   "outputs": [],
   "source": [
    "a = torch.ones((2, 3))\n",
    "b = torch.full((2, 3), 8)\n",
    "\n",
    "print(f'A = {a}\\n\\nB = {b}\\n\\nA + B = {a + b}\\n\\nA * B = {a * b}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0H7nwDpZS6q"
   },
   "source": [
    "Умножать тензоры матрично"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V6qiT8_qZPHt",
    "outputId": "cd6386af-2735-4026-8e9d-33566ae3d272"
   },
   "outputs": [],
   "source": [
    "a = torch.ones((2, 3))\n",
    "b = torch.full((3, 2), 8.)\n",
    "\n",
    "\n",
    "print(f'A = {a}\\n\\nB = {b}\\n\\nA @ B = {a @ b}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14QyAvo0a0gH"
   },
   "source": [
    "Применять агрегирующие функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VAQDgur3az96",
    "outputId": "fefdbb5b-4119-42af-b9bb-a2eb4ba7206a"
   },
   "outputs": [],
   "source": [
    "a = torch.ones((4, 3))\n",
    "\n",
    "\n",
    "print(a, 'Тензор A\\n')\n",
    "print(f'Усреденение по столбцам A  = {a.mean(dim=0)}\\n')\n",
    "print(f'Сумма по строчкам A  = {a.sum(dim=1)}\\n')\n",
    "print(f'Кумулятивная сумма по столбцам A:\\n{a.cumsum(dim=0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LdRB0Q0FebGd"
   },
   "source": [
    "### Переход от `torch.tensor` к `numpy.ndarray`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_qWGHnlezbP"
   },
   "source": [
    "Если вам требуется перевести тензор в массив numpy, то сделать это можно вызовом метода `numpy()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jG51VqSseadz",
    "outputId": "82b5600e-3144-4bb0-ed19-36d710565f26"
   },
   "outputs": [],
   "source": [
    "t = torch.ones((2, 3))\n",
    "\n",
    "print(type(t), 't - тензор\\n')\n",
    "print(type(t.numpy()), 't стал массивом numpy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iO5vz4TbH7BC"
   },
   "source": [
    "**ЗАДАЧА 1**\n",
    "\n",
    "Пользуясь функциями `torch.tensor` создайте **один** тензор, и посчитайте с его помощью сумму чисел от 1 до 100. Затем посчитайте суммы чисел в каждом десятке (от 1 до 10, от 11 до 20 и так далее)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f8_J6mPcMSh5",
    "outputId": "a22e3110-09fa-4b2e-fbf2-5108dc741191"
   },
   "outputs": [],
   "source": [
    "A_tensor = torch.tensor(range(1, 101))\n",
    "A_total = torch.sum(A_tensor)\n",
    "A_decade = torch.sum(A_tensor.view(10, 10), dim = 1)\n",
    "\n",
    "print(f\"Исходный тензор: {A_tensor}\")\n",
    "print(f\"Сумма всех чисел от 1 до 100: {A_total}\")\n",
    "print(f\"Суммы чисел в каждом десятке: {A_decade.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T4_b7OJL8hjX"
   },
   "source": [
    "Все же некоторые названия методов отличаются от `numpy`-евских. Полной совместимости с `numpy` пока нет, но от версии к версии разрыв сокращается, и придется снова запоминать новые названия для некоторых методов.\n",
    "\n",
    "Например, PyTorch имеет другое написание стандартных типов\n",
    " * `x.astype('int64') -> x.type(torch.LongTensor)`\n",
    "\n",
    "\n",
    "Для более подробного ознакомления можно посмотреть на <a href=\"https://github.com/torch/torch7/wiki/Torch-for-Numpy-users\" target=\"_blank\">табличку</a> перевода методов из `numpy` в `torch`, а также заглянуть в <a href=\"http://pytorch.org/docs/master/\" target=\"_blank\">документацию</a>. Также при возникновении проблем часто помогает зайти на <a href=\"https://discuss.pytorch.org/\" target=\"_blank\">pytorch forums</a>. Можете также глянуть эту <a href=\"https://qudata.com/ml/ru/NN_Base_Torch.html\" target=\"_blank\">шпаргалку</a> по тензорам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8kzxCLDZ8hjY"
   },
   "source": [
    "**ЗАДАЧА 2**\n",
    "\n",
    "Нарисуйте по сетке данную кривую на графике, используя `torch`:\n",
    "\n",
    "$$x(t) = 2 \\cos t + \\sin 2t \\cos 60t,$$\n",
    "\n",
    "$$y(t) = \\sin 2t + \\sin 60t.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 718
    },
    "id": "gQ33m_TF8hjY",
    "outputId": "6cad33eb-d893-4103-8bda-d211dd45583b"
   },
   "outputs": [],
   "source": [
    "t = torch.linspace(0, 2 * torch.pi, 1000)\n",
    "\n",
    "x_t = 2 * torch.cos(t) + torch.sin(2 * t) * torch.cos(60 * t)\n",
    "y_t = torch.sin(2 * t) + torch.sin(60 * t)\n",
    "\n",
    "plt.figure(figsize = (8, 8))\n",
    "plt.plot(x_t, y_t)\n",
    "\n",
    "plt.title('График кривой x(t) и y(t)')\n",
    "plt.xlabel('x(t)')\n",
    "plt.ylabel('y(t)')\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8RP7x-YIYvQL"
   },
   "source": [
    "Заметим, что библиотека `matplotlib` справляется с отображением `pytorch`-тензоров, и дополнительных преобразований делать не нужно.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tbm0PRJpBjzt"
   },
   "source": [
    "## Вычисление на видеокарте"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0z8qxbOyBxf2"
   },
   "source": [
    "Если, тензоры в PyTorch и массивы в NumPy так похожи, то зачем нам вообще нужен PyTorch? Для этого надо вспомнить, что нейросети бывают огромных размеров, а проводить операции над большими матрицами вычислетельно дорого. Давайте перемножим две матрицы размером 10000 на 10000 (слои трансформеры в GPT-3 содержат матрицы ещё большего размера):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x9sbA49IFCOF"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "n = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NCmiw0AbE4zb"
   },
   "outputs": [],
   "source": [
    "a = np.ones((n, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kvRKgxdgE_4h",
    "outputId": "43d43a2c-d7e9-4eaf-8588-0a3d7bb8b513"
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "np.dot(a, a)\n",
    "\n",
    "end = time.time() - start\n",
    "\n",
    "print(end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwQ_FYJQEVOf"
   },
   "source": [
    "Вроде бы терпимо. Теперь представьте, что данную операцию надо совершить десятки тысяч раз, и над несколькими матрицами. Уже ставится грустно. Однако, матричные вычисления хорошо распараллеливаются и тут на сцену выходит PyTorch. В нём реализовано распараллеливание вычислений с помощью видеокарт, что позволяет в разы ускорить работу с матрицами в разы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mwuBBBWG5b4"
   },
   "source": [
    "Стандартом является использование видеокарт NVidia и языка CUDA (хотя есть поддержка и других производителей). Для начала вычислений на видеокарте надо перенести на неё тензор, вызвав метод `cuda`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Op7oSiSBxSg"
   },
   "outputs": [],
   "source": [
    "t = torch.ones((n, n))\n",
    "\n",
    "device = 'cuda'\n",
    "t = t.to(device) #переносим тензор на видеокарту"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rKEyxIekH04P",
    "outputId": "39a58f94-04f5-4e70-fe84-4a3ed54cb7e4"
   },
   "outputs": [],
   "source": [
    "t.device #проверим, что перенеслось"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lU4tcBrXIKI6"
   },
   "source": [
    "Теперь можно производить вычисления с использованием видеокарты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rrN8LlwgICDf",
    "outputId": "cbd163c0-b022-40cb-b8fe-fd6c948e4704"
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "torch.mul(t, t)\n",
    "\n",
    "end = time.time() - start\n",
    "\n",
    "print(end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lfJqAG0yIn4v"
   },
   "source": [
    "Как видите, ускорение в несколько тысяч раз!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ABkJg3WH7BF"
   },
   "source": [
    "## Дифференцирование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_aXtxH9IS-Xc"
   },
   "source": [
    "\n",
    "\n",
    "Обучение нейронной сети опирается на алгоритм обратного распространения ошибки, для этого необходимо уметь строить граф вычислений и дифференцировать его. Как вы уже догодались, PyTorch с эти успешно справляется. Посмотрим как это происходит"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H5y3MSPgU14B"
   },
   "source": [
    "Установим библиотек для визуализации графа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DlXLt-0eaIc8",
    "outputId": "19333b6f-6fbb-4153-ffbd-49dc4cb4a2e0"
   },
   "outputs": [],
   "source": [
    "!pip install torchviz\n",
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S1XOEy4mVeT0"
   },
   "outputs": [],
   "source": [
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "502P2CBDWzPf"
   },
   "source": [
    "Для того, чтобы добавить тензор к вычислительному графу необходимо при его создании установить параметр `requires_grad=True`. Обратите внимание, что только тензоры вещественных чисел  можно привязать к графу"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMKPfdTrVnab"
   },
   "source": [
    "Построим вычислительный граф для следующей функции\n",
    "\n",
    "$$f(x, y)=x^2 * y + 10x^{\\sin(y)}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qpk1DuwBVnMB"
   },
   "outputs": [],
   "source": [
    "x = torch.tensor(1, requires_grad=True, dtype=torch.float)\n",
    "y = torch.tensor(1, requires_grad=True, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uWm5OUVaX5-Q"
   },
   "outputs": [],
   "source": [
    "f = x ** 2 * y + 10 * x ** torch.sin(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "4x5q7JM9X9d9",
    "outputId": "3451c5f7-09f7-4aee-fd01-bb22e761a20f"
   },
   "outputs": [],
   "source": [
    "make_dot(f, params={'x': x, 'y': y, 'f': f})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VyyQOTlCZVbU"
   },
   "source": [
    "Для того, чтобы посчитать градиент функции по всем параметрам, надо всего лишь вызвать метод `backward`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xlNRpdqijWr1"
   },
   "outputs": [],
   "source": [
    "f.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKHlhRtjjrjC"
   },
   "source": [
    "Получившиеся градиенты являются атрибутами параметров функции. Чтобы посмотреть их надо посмотреть атрибут `grad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zO-IlyWDjrEf",
    "outputId": "16cb2b80-96b1-4dac-faf4-37e52fa0d18b"
   },
   "outputs": [],
   "source": [
    "y.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xaktdT5CjqaD",
    "outputId": "8c90e3bd-d155-4a64-842e-5ec275751aca"
   },
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtEyzxQCncL3"
   },
   "source": [
    "PyTorch устроен так, что градиенты накапливаются"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3xGlWEonnc3b"
   },
   "outputs": [],
   "source": [
    "f = x ** 2 * y + 10 * x ** torch.sin(y)\n",
    "f.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QXEGDzs1n6oG",
    "outputId": "4de8d1bc-23ba-4a37-8134-cc90b8ff0426"
   },
   "outputs": [],
   "source": [
    "y.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HxL1m7gJn80p",
    "outputId": "749248b9-5dab-4297-df12-3f27e9383102"
   },
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "atn99ek-ovhx"
   },
   "source": [
    "Поэтому прежде чем повторять процедуру дифференцирования необходиомо занулить градиенты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QzQ4EcIao_Am",
    "outputId": "c7ac9279-d581-4caa-e90a-c0163762b02d"
   },
   "outputs": [],
   "source": [
    "x.grad.zero_()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-TSpHR_GpMB2",
    "outputId": "8cea90fe-ae01-42f7-f7a1-f346ba63c50c"
   },
   "outputs": [],
   "source": [
    "y.grad.zero_()\n",
    "y.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7Y8XPk9qkX-"
   },
   "source": [
    "Чтобы отсоеденить тензор от вычислительного графа и перестать накапливать градиенты воспользуйтесь методом `detach()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Aaez18OqTHA",
    "outputId": "306c85e0-1af1-4b72-eb99-ffc42998f281"
   },
   "outputs": [],
   "source": [
    "x.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kYyO8KtJriLo"
   },
   "source": [
    "Чтобы создать копию тензора можно воспользоваться методом `clone()`, при этом склонированный тензор будет присоединён к графу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jcKsZ6Tkrhhz",
    "outputId": "30ef9f12-067d-4b99-9c63-3dfb523d9cb6"
   },
   "outputs": [],
   "source": [
    "a = y.clone()\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hetC9Ei6L_27"
   },
   "source": [
    "**ЗАДАНИЕ 3**\n",
    "\n",
    "Посчитайте производную функции\n",
    "\n",
    "$$f(x, y, z)=\\frac{x^2 + y^2}{\\exp z}$$\n",
    "\n",
    "по переменной $x$ в точке (1, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fCVBqYbxOyMp",
    "outputId": "1b6a2c3c-fc62-4a81-b391-344d0aefb06e"
   },
   "outputs": [],
   "source": [
    "x = torch.tensor(1.0, requires_grad = True)\n",
    "y = torch.tensor(1.0, requires_grad = True)\n",
    "z = torch.tensor(0.0, requires_grad = True)\n",
    "\n",
    "f = (x**2 + y**2) / torch.exp(z)\n",
    "\n",
    "f.backward()\n",
    "\n",
    "print(f\"Градиент по x: {x.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64Rj81Jy8hji"
   },
   "source": [
    "## Простой пример обучения нейронной сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFPUqOB3pb4p"
   },
   "source": [
    "Теперь, можно приступить к самому главному назначению PyTorch и обучить нейросеть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2WAHjImo8hjj"
   },
   "source": [
    "\n",
    "### Цикл обучения модели\n",
    "\n",
    "Посмотрим на общий процесс обучения. Пусть задана нейронная сеть $f(x)$, параметризуемая обучаемыми параметрами $\\theta$. Для входных данных $x$ модель возращает $\\widehat{y}=f(x)$. Для обучения модели необходимо задать оптимизируемую функцию (функцию ошибки, лосс) $L(y, \\widehat{y})$, которую следует минимизовать.\n",
    "\n",
    "\n",
    "Процесс обучения задается следующим образом.\n",
    " * **Прямой проход / Forward pass:** <br>\n",
    "     Считаем $\\widehat{y}=f(x)$ для входных данных $x$. <br>\n",
    " * **Вычисление оптимизируемой функции:**<br>\n",
    "     Вычисляем оптимизируемую функцию $L(y, \\widehat{y})$. <br>\n",
    " * **Обратный проход / Backward pass:** <br>\n",
    "     Считаем градиенты по всем обучаемым параметрам $\\frac{\\partial L}{\\partial \\theta}$. <br>\n",
    " * **Шаг оптимизации:** <br>\n",
    "     Делаем шаг градиентного спуска, обновляя все обучаемые параметры. <br>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02QbI8x9osL2"
   },
   "source": [
    "### Линейная регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHtIECY8p9N3"
   },
   "source": [
    "В лекциях показано, что линейную регрессию можно представить как частный случай нейрона с тождественной функцией активации.\n",
    "\n",
    "Сделаем одномерную линейную регрессию на датасете <a href=\"http://lib.stat.cmu.edu/datasets/boston\" target=\"_blank\">boston</a>. Этот датасет представляет собой набор данных конца 70-х годов прошлого века для предсказания цены недвижимости в Бостоне.\n",
    "\n",
    "Скачиваем данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KNM9p7z4rQvl"
   },
   "outputs": [],
   "source": [
    "def load_boston():\n",
    "    # ссылка для скачивания данных\n",
    "    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "    # собираем таблицу данных\n",
    "    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "    # выделяем признаки и таргет\n",
    "    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "    target = raw_df.values[1::2, 2]\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H7Tbvoo6r5CK"
   },
   "outputs": [],
   "source": [
    "data, target = load_boston()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbxWbR8HdLIS"
   },
   "source": [
    "Будем рассматривать зависимость таргета, т.е. медианной стоимости домов в тысячах долларов, от последнего признака, т.е. процента населения людей с низким уровнем дохода."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "id": "4_oACFuNtxxf",
    "outputId": "0d953a53-4cd3-42f8-b610-e5a2dd05f1c5"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(data[:, -1], target, alpha=0.7)\n",
    "plt.xlabel('% населения с низким уровнем дохода')\n",
    "plt.title('Медианная стоимость домов в тыс. $');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYOlvM88uxVw"
   },
   "source": [
    "В данном случае предсказание модели задается следующим образом: $$\\widehat{y}(x) = wx + b,$$\n",
    "где $w, b \\in \\mathbb{R}$ &mdash; обучаемые параметры модели. Это обычная линейная модель, и с ней мы уже работали <a href=\"https://miptstats.github.io/courses/ad_fivt/linreg_sklearn.html\" target=\"_blank\">ранее</a>.\n",
    "\n",
    "Объявляем обучаемые параметры. Также задаем признак $X$ и таргет $Y$ в виде `torch`-тензоров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BdC9gdtZuLM4"
   },
   "outputs": [],
   "source": [
    "# создаем два тензора размера 1 с заполнением нулями,\n",
    "# для которых будут вычисляться градиенты\n",
    "w = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# Данные оборачиваем в тензоры, по которым не требуем вычисления градиента\n",
    "x = torch.FloatTensor(data[:, -1] / 10)\n",
    "y = torch.FloatTensor(target)\n",
    "\n",
    "# по-другому:\n",
    "# x = torch.tensor(boston.data[:, -1] / 10, dtype=torch.float32)\n",
    "# y = torch.tensor(boston.target, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1lcCp-j3vqcK"
   },
   "source": [
    "Зададим оптимизируемую функцию / функцию ошибки / лосс &mdash; <a href=\"https://miptstats.github.io/courses/ad_fivt/linreg_sklearn.html#3.-Тестирование-и-оценка-качества\" target=\"_blank\">MSE</a>:\n",
    "\n",
    "$$\n",
    "\\mathrm{MSE}(\\widehat{y}, y) = \\frac{1}{n} \\sum_{i=1}^n \\left(\\widehat{y}_i - y_i\\right)^2.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h7yrmAEzv1_0"
   },
   "outputs": [],
   "source": [
    "def optim_func(y_pred, y_true):\n",
    "    return torch.mean((y_pred - y_true) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2mjeFLwadjhO"
   },
   "source": [
    "После того, как мы посчитаем результат применения этой функции к нашим данным, нам необходимо посчитать градиенты по всем обучаемым параметрам, чтобы затем сделать шаг градиентного спуска. В этом нам поможет функция `backward`. Вызвав `backward` для результата подсчета функции ошибки `loss`, мы сделаем обратный проход по всему графу вычислений и посчитаем градиенты лосса по всем обучаемым параметрам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DQ7erDUSdgw8"
   },
   "outputs": [],
   "source": [
    "# Прямой проход\n",
    "y_pred = w * x + b\n",
    "\n",
    "# Вычисление лосса\n",
    "loss = optim_func(y_pred, y)\n",
    "\n",
    "# Вычисление градиентов\n",
    "# с помощью обратного прохода по сети\n",
    "# и сохранение их в памяти сети\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBVjjTu9dnr4"
   },
   "source": [
    "Здесь `loss` &mdash; значение функции MSE, вычисленное на этой итерации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3cF5x-GhdgtA",
    "outputId": "e27df293-7e05-4a20-f907-7dcec9d35a7b"
   },
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQgYctV9dsd3"
   },
   "source": [
    "К градиентам для обучаемых параметров, которые требуют градиента (`requires_grad=True`), теперь можно обратиться следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kn4q9G46dgqZ",
    "outputId": "145a912d-2bd3-4013-f326-0f9c88671319"
   },
   "outputs": [],
   "source": [
    "print(\"dL/dw =\", w.grad)\n",
    "print(\"dL/db =\", b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0JIOwcGWd0A8"
   },
   "source": [
    "Если мы посчитаем градиент $M$ раз, то есть $M$ раз вызовем `loss.backward()`, то градиент будет накапливаться (суммироваться) в параметрах, требующих градиента. Иногда это бывает удобно.\n",
    "\n",
    "Убедимся на примере, что именно так все и работает."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QpNTaOKqdgiY",
    "outputId": "2dc0911d-b058-491f-a5a8-cdeaff51882a"
   },
   "outputs": [],
   "source": [
    "y_pred = w * x + b\n",
    "loss =  optim_func(y_pred, y)\n",
    "loss.backward()\n",
    "\n",
    "print(\"dL/dw =\", w.grad)\n",
    "print(\"dL/b =\", b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIwpQpACd5Lr"
   },
   "source": [
    "Видим, что значения градиентов стали в 2 раза больше, за счет того, что мы сложили одни и те же градиенты 2 раза."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lBFVSRF6d9sB"
   },
   "source": [
    "Если же мы не хотим, чтобы градиенты суммировались, то нужно **занулять\n",
    "градиенты** между итерациями после того как сделали шаг градиентного спуска.\n",
    "Это можно сделать с помощью функции `zero_` для градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hh5zDZH8d6g1",
    "outputId": "da412537-2634-495b-e72f-25113134f358"
   },
   "outputs": [],
   "source": [
    "w.grad.zero_()\n",
    "b.grad.zero_()\n",
    "w.grad, b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FDchenDBeCq-"
   },
   "source": [
    "Напишем функцию визуализации процесса обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GAsouKKUeENK"
   },
   "outputs": [],
   "source": [
    "def show_progress(x, y, y_pred, loss):\n",
    "    '''\n",
    "    Визуализация процесса обучения.\n",
    "\n",
    "    x, y -- объекты и таргеты обучающей выборки;\n",
    "    y_pred -- предсказания модели;\n",
    "    loss -- текущее значение ошибки модели.\n",
    "    '''\n",
    "\n",
    "    # Избавимся от градиентов перед отрисовкой графика\n",
    "    y_pred = y_pred.detach()\n",
    "\n",
    "    # Превратим тензор размерности 0 в число, для красивого отображения\n",
    "    loss = loss.item()\n",
    "\n",
    "    # Стираем предыдущий вывод в тот момент, когда появится следующий\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    # Строим новый график\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.scatter(x, y, alpha=0.75)\n",
    "    plt.scatter(x, y_pred, color='orange', linewidth=5)\n",
    "    plt.xlabel('% населения с низким уровнем дохода')\n",
    "    plt.title('Медианная стоимость домов в тыс. $')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"MSE = {loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5FAvkdTeK3-"
   },
   "source": [
    "**ЗАДАЧА 4**\n",
    "\n",
    "Построим нейронную сеть из двух слоев:\n",
    "\n",
    "$$\\widehat{y}(x) = w_2u(x) + b_2,$$\n",
    "\n",
    "$$u(x) = \\sigma(w_1x + b_1),$$\n",
    "\n",
    "$$\\sigma(x) = \\text{ReLU}(x) = \\begin{equation*}\\begin{cases}x, \\; x \\ge 0, \\\\ 0, \\; \\text{иначе,} \\end{cases} \\end{equation*}$$\n",
    "\n",
    "$w_1, b_1 \\in \\mathbb{R}$ &mdash; обучаемые параметры первого слоя, $w_2, b_2 \\in \\mathbb{R}$ &mdash; обучаемые параметры второго слоя, $\\sigma(x)$ &mdash; функция активации, в данном случае мы выбрали `ReLU`. Можно заметить, что это функция не удовлетворяет условиям теоремы Цыбенко, тем не менее на практике она часто применяется для нейронных сетей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 678
    },
    "id": "k9eOguSjeXeO",
    "outputId": "2eee4d98-8a07-43d9-a2d9-0099b8578e2f"
   },
   "outputs": [],
   "source": [
    "# Инициализация параметров\n",
    "w0 = torch.randn(1, requires_grad = True)\n",
    "b0 = torch.randn(1, requires_grad = True)\n",
    "w1 = torch.randn(1, requires_grad = True)\n",
    "b1 = torch.randn(1, requires_grad = True)\n",
    "\n",
    "# Функция активации\n",
    "def act_func(x):\n",
    "    return x * (x >= 0)\n",
    "\n",
    "# Количество итераций\n",
    "num_iter = 1000\n",
    "\n",
    "# Скорость обучения для параметров\n",
    "lr_w = 0.01\n",
    "lr_b = 0.05\n",
    "\n",
    "for i in range(num_iter):\n",
    "\n",
    "    # Forward pass: предсказание модели\n",
    "    u = act_func(w0 * x + b0)\n",
    "    y_pred = w1 * u + b1\n",
    "\n",
    "    # Вычисление оптимизируемой функции (MSE)\n",
    "    loss = optim_func(y_pred, y)\n",
    "\n",
    "    # Bakcward pass: вычисление градиентов\n",
    "    loss.backward()\n",
    "\n",
    "    # Оптимизация: обновление параметров\n",
    "    with torch.no_grad():\n",
    "        w0 -= lr_w * w0.grad\n",
    "        b0 -= lr_b * b0.grad\n",
    "        w1 -= lr_w * w1.grad\n",
    "        b1 -= lr_b * b1.grad\n",
    "\n",
    "    # Зануление градиентов\n",
    "    w0.grad.zero_()\n",
    "    b0.grad.zero_()\n",
    "    w1.grad.zero_()\n",
    "    b1.grad.zero_()\n",
    "\n",
    "    # График + вывод MSE через каждые 5 итераций\n",
    "    if (i + 1) % 5 == 0:\n",
    "        show_progress(x, y, y_pred, loss)\n",
    "\n",
    "        if loss.item() < 33:\n",
    "            print(\"Готово!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sfdc03DaZ551"
   },
   "source": [
    "\n",
    "\n",
    "# Готовые модули из PyTorch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKSjnmGLZ552"
   },
   "source": [
    "Для ознакомления с высокоуровневым интерфейсом будем решать задачу классификации картинок на 10 классов на датасете CIFAR10 из 60k картинок размера 3x32x32.\n",
    "\n",
    "Скачаем картинки и посмотрим на них:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EPjT_aB3Z552",
    "outputId": "a6ca13b7-fd3a-4564-de1e-2b63ed6f6b74"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/riknel/Ml_lectures/master/cifar.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gvXdepehZ553",
    "outputId": "d94e7a62-7738-4c47-aee7-860e5f44d721"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from cifar import load_cifar10\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10('cifar_data')\n",
    "\n",
    "class_names = np.array([\n",
    "    'airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "    'dog', 'frog', 'horse', 'ship', 'truck'\n",
    "])\n",
    "\n",
    "print(type(X_train), type(y_train))\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 805
    },
    "id": "0nphUboMZ553",
    "outputId": "8b87bc4e-367f-41d3-80c8-0a5ea0c9b057"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "for i in range(12):\n",
    "    plt.subplot(3, 4, i + 1)\n",
    "    plt.axis('off')\n",
    "    plt.title(class_names[y_train[i]])\n",
    "    plt.imshow(np.transpose(X_train[i],[1, 2, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJTI8-xNuNn5"
   },
   "source": [
    "Обучение проходит по батчам. Поэтому нам нужно либо самим написать генератор для батчей, либо использовать уже написанный за нас класс `DataLoader`. `DataLoader` принимает в аргументах размер батча и также ему можно сказать, нужно ли перемешивать данные. Пока что сами напишем генератор:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1DsyODV9uSZZ"
   },
   "outputs": [],
   "source": [
    "def batch_generator(X, y, batchsize, device, shuffle=True):\n",
    "    '''\n",
    "        Генерирует tuple из батча объектов и их меток\n",
    "        X: np.ndarray -- выборка\n",
    "        y: np.ndarray -- таргет\n",
    "        batchsize: int -- размер батча\n",
    "        device: str -- утсройство, на котором будут производиться вычисления\n",
    "        shuffle: bool -- перемешивать выборку или нет\n",
    "    '''\n",
    "\n",
    "    indices = np.arange(len(X))\n",
    "\n",
    "    # Во время обучения перемешиваем, во время тестирования - нет\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(indices)\n",
    "\n",
    "    # Идем по всем данным с шагом batchsize.\n",
    "    # Возвращаем start: start + batchsize объектов на каждой итерации\n",
    "    for start in range(0, len(indices), batchsize):\n",
    "        ix = indices[start: start + batchsize]\n",
    "\n",
    "        # Переведем массивы в соотв. тензоры.\n",
    "        # Для удобства переместим выборку на наше устройство (GPU).\n",
    "        yield torch.FloatTensor(X[ix]).to(device), torch.LongTensor(y[ix]).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QxylH-FguYwt"
   },
   "source": [
    "Главной абстракцией в PyTorch является `torch.nn.Module`.  \n",
    "По сути модуль можно понимать как нейронную сеть или ее какую-то часть. Каждый стандартный слой в PyTorch-е наследуются от `torch.nn.Module`.\n",
    "\n",
    "Модуль это нечто, что имеет метод `forward` и, возможно, `backward`. Но вообще `backward` является автоматическим (`autograd`).  Кроме того, модуль может содержать в себе другие модули."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MlOVxBbaub59",
    "outputId": "61a5809e-c924-4ac0-87c2-8506492bd022"
   },
   "outputs": [],
   "source": [
    "print(nn.Module.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qR-qt21aue1t"
   },
   "source": [
    "Часто, когда нейронная сеть не слишком сложная и последовательная, удобно пользоваться `nn.Sequential()` &mdash; последовательный контейнер из модулей. Модули, входящие в этот контейнер будут выполнятся последовательно один за другим, то есть выход одного подается на вход следующему."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Qg_YEL_uif1"
   },
   "source": [
    "Теперь напишем саму модель, которая\n",
    "будет возвращать **логиты (logits)**. Логиты - это то, что получается **до применения SoftMax** для получения вероятностей в интервале 0-1. Они были у вас в логистической регрессии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vZRGN9ZCufWB"
   },
   "outputs": [],
   "source": [
    "# Создаем последовательную нейронную сеть\n",
    "model = nn.Sequential()\n",
    "\n",
    "# Преобразуем входной тензор размера batch_size x n1 x n2 x ... x nm\n",
    "# к виду batch_size x n, где  n = n1 x n2 x ... x nm\n",
    "model.add_module('flatten', nn.Flatten())\n",
    "\n",
    "# Добавляем линейный слой с выходным размером 64.\n",
    "# Размер входа равен произведению размерностей данных.\n",
    "model.add_module('linear_1', nn.Linear(3 * 32 * 32, 64))\n",
    "\n",
    "# Добавляем функцию активации ReLU\n",
    "model.add_module('relu', nn.ReLU())\n",
    "\n",
    "# Добавляем еще 1 линейный слой с выходным размером 10,\n",
    "# равным количеству классов, на выходе получаем логиты\n",
    "model.add_module('linear_2', nn.Linear(64, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jfn18yV3uub4"
   },
   "outputs": [],
   "source": [
    "# По-другому с именами слоев\n",
    "model = nn.Sequential(OrderedDict([\n",
    "    ('flatten', nn.Flatten()),\n",
    "    ('linear_1', nn.Linear(3 * 32 * 32, 64)),\n",
    "    ('relu', nn.ReLU()),\n",
    "    ('linear_2', nn.Linear(64, 10))\n",
    "]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lmwg0o1KuymA"
   },
   "outputs": [],
   "source": [
    "# По-другому без имен слоев\n",
    "model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(3 * 32 * 32, 64),  # 3072 x 64\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aoglhMKJu1GK",
    "outputId": "ecfe16b0-8734-40d6-f46e-516b5f61f951"
   },
   "outputs": [],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHW251CZvFVS"
   },
   "source": [
    "*Примечание:* Если вы используете имена для слоев при создании контейнера, то имена должны быть разными, иначе при встрече слоя с уже существующим именем, предыдущий слой с таким именем будет перезаписан на новый."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oXIJJ5h5vr8S"
   },
   "source": [
    "У полученной модели можно посмотреть на все ее обучаемые параметры : `model.parameters()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LeKktr6Hvt_0",
    "outputId": "463cb97f-ea46-4d4c-9de5-e733a6933237"
   },
   "outputs": [],
   "source": [
    "print(\"Weight shapes:\", [w.shape for w in model.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V3xsqKZmvHQI"
   },
   "source": [
    "Мы создали модель, то есть научились по какому-либо входу получать выход модели. Чтобы обучить данную модель, нам нужно минимизировать эмпирический риск, то есть функцию потерь (\"лосс\"). Для этого определим его.\n",
    "\n",
    "### Объявление функции потерь (лосса)\n",
    "\n",
    "Для разных задач необходимо использовать разные функции потерь.\n",
    "\n",
    "Пусть $n$ &mdash; размер выборки.\n",
    "- `LogLoss / BinaryCrossEntropy / BCE`: *бинарная классификация*.  \n",
    "Пусть $\\widehat{y_i} \\in (0,1), y \\in \\{0,1\\}$, тогда\n",
    "$$\n",
    "\\mathcal{L}(\\widehat{y}, y) = -\\frac{1}{n} \\sum_{i = 1}^n \\Big[ y_i \\log{\\widehat{y}_i} + (1 - y_i) \\log{(1 - \\widehat{y}_i)}\\Big]\n",
    "$$\n",
    "Обратите внимание, что в `pytorch` присутствует два варианта реализации этого лосса `BCELoss` и `BCEWithLogitsLoss`. Первый вариант принимает на вход предсказанную вероятность класса 1 (т. е. выход сети после применения сигмоиды), второй же - логит (т. е. выход сети до применения сигмоиды, а преобразование делается внутри). Второй вариант является более вычислительно стабильным.\n",
    "\n",
    "- `CrossEntropy`: *многоклассовая классификация*.  \n",
    "Пусть $\\widehat{y}_{ij} \\in (0,1)^n, y_{ij} \\in \\{0,1\\}^n$, $K$ $-$ количество классов, тогда\n",
    "$$\n",
    "\\mathcal{L}(\\widehat{y}, y) = -\\frac{1}{n} \\sum_{i = 1}^n  \\sum_{j=1}^K y_{ij} \\log\\widehat{y}_{ij}\n",
    "$$\n",
    "- `MSELoss`: *регрессия*  \n",
    "Пусть $\\widehat{y}, y \\in \\mathbb{R}$, тогда\n",
    "$$\n",
    "\\mathcal{L}(\\widehat{y}, y) = \\frac{1}{n} \\sum_{i = 1}^n  (\\widehat{y}-y)^2\n",
    "$$\n",
    "\n",
    "Мы имеем дело с многоклассовой классификацией, поэтому берем CrossEntropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rp43tfbJv2Yi"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zofz9BZvv8T8"
   },
   "source": [
    "###  Подбираем оптимизатор\n",
    "\n",
    "В `torch.optim` лежит много разных уже готовых оптимизаторов таких как SGD, RMSprop, Adam и прочие. О них мы поговорим подробнее на следующей лекции.\n",
    "\n",
    "Оптимизатор принимает набор тензоров, по которым он будет считать градиенты и которые будет оптимизировать. Обычно это все параметры модели поэтому обычно передаем `model.parameters()`. <br>\n",
    "Сначала нам нужно сделать обратный проход посчитав все градиенты, потом мы делаем шаг и уже в конце зануляем градиенты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1UbTcdulwFvE"
   },
   "outputs": [],
   "source": [
    "opt = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# loss.backward()      # обратный проход, считаем градиенты\n",
    "# opt.step()           # делаем шаг градиентного спуска\n",
    "# opt.zero_grad()      # зануляем градиенты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JeoH96gZwQWf"
   },
   "source": [
    "Теперь у нас все готово для обучения.\n",
    "\n",
    "В фазе обучения мы вызываем метод `train(True)` у модели, чтобы перевести ее в фазу обучения: `model.train(True)`. В фазе тестирования ставим `model.train(False)` или `model.eval()`.\n",
    "Это влияет на:\n",
    "- поведение Dropout слоев\n",
    "- поведение BatchNorm слоев\n",
    "\n",
    "Сейчас эти слои мы не используем, но уже держим это в голове.\n",
    "\n",
    "В фазе тестирования будем использовать контекстный менеджр `torch.no_grad`, который отключает возможность подсчета градиентов. Это позволяет более экономно использовать память.\n",
    "\n",
    "По ходу обучения будем сохранять лучшую модель по метрике на валидации. Это просто делается с помощью метода `torch.save`. Рекомендуется сохранять не сам класс модели, а ее состояние (значения параметров). Подробности можно посмотреть [здесь](https://pytorch.org/tutorials/beginner/saving_loading_models.html).  Выгрузить модель можно с помощью метода `torch.load()`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fY128siTwrEX",
    "outputId": "af7000c5-15ff-4b1e-90b0-85941c7e84fb"
   },
   "outputs": [],
   "source": [
    "num_epochs = 30  # общее кол-во полных проходов (\"эпох\") по обучаемым данным\n",
    "batch_size = 64  # кол-во объектов в одном батче\n",
    "\n",
    "num_train_batches = len(X_train) // batch_size\n",
    "num_val_batches = len(X_val) // batch_size\n",
    "\n",
    "history = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "best_val_acc = 0.\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Устанавливаем поведение dropout / batch_norm  в обучение\n",
    "    model.train(True)\n",
    "\n",
    "    # На каждой \"эпохе\" делаем полный проход по данным\n",
    "    for X_batch, y_batch in batch_generator(X_train, y_train, batch_size, device=device):\n",
    "\n",
    "        # Обучаемся на батче (одна \"итерация\" обучения нейросети)\n",
    "        logits = model(X_batch)\n",
    "        loss = criterion(logits, y_batch)\n",
    "        # Обратный проход, шаг оптимизатора и зануление градиентов\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # Используйте методы тензоров:\n",
    "        # detach -- для отключения подсчета градиентов\n",
    "        # cpu -- для перехода на cpu\n",
    "        # numpy -- чтобы получить numpy массив\n",
    "        train_loss += loss.detach().cpu().numpy()\n",
    "        y_pred_np = np.argmax(logits.detach().cpu().numpy(), axis=1)\n",
    "        y_batch_np = y_batch.cpu().numpy()\n",
    "        train_acc += (y_batch_np == y_pred_np).sum()\n",
    "\n",
    "    # Подсчитываем лоссы и сохраням в \"историю\"\n",
    "    train_loss /= num_train_batches\n",
    "    train_acc /= num_train_batches * batch_size\n",
    "    history['loss']['train'].append(train_loss)\n",
    "    history['acc']['train'].append(train_acc)\n",
    "\n",
    "    # Устанавливаем поведение dropout / batch_norm  в тестирование\n",
    "    model.eval()\n",
    "\n",
    "    # Полный проход по валидации\n",
    "    with torch.no_grad(): # Отключаем подсчет градиентов, то есть detach не нужен\n",
    "        for X_batch, y_batch in batch_generator(X_val, y_val, batch_size, device=device):\n",
    "            logits = model(X_batch)\n",
    "            loss = criterion(logits, y_batch)\n",
    "\n",
    "            val_loss += loss.cpu().numpy().sum()\n",
    "            y_pred_np = np.argmax(logits.cpu().numpy(), axis=1)\n",
    "            y_batch_np = y_batch.cpu().numpy()\n",
    "            val_acc += (y_batch_np == y_pred_np).sum()\n",
    "\n",
    "    # Подсчитываем лоссы и сохраням в \"историю\"\n",
    "    val_loss /= num_val_batches\n",
    "    val_acc /= num_val_batches * batch_size\n",
    "    history['loss']['val'].append(val_loss)\n",
    "    history['acc']['val'].append(val_acc)\n",
    "\n",
    "    # Сохраняем лучшую модель по метрике на валидации\n",
    "    if val_acc > best_val_acc:\n",
    "        torch.save(model.state_dict(), 'first_model.pth')\n",
    "\n",
    "    # Печатаем результаты после каждой эпохи\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss (in-iteration): \\t{:.6f}\".format(train_loss))\n",
    "    print(\"  validation loss (in-iteration): \\t{:.6f}\".format(val_loss))\n",
    "    print(\"  training accuracy: \\t\\t\\t{:.2f} %\".format(train_acc * 100))\n",
    "    print(\"  validation accuracy: \\t\\t\\t{:.2f} %\".format(val_acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GXeZaO3Jwt19"
   },
   "source": [
    "Построим кривые обучения (learning curves):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 649
    },
    "id": "icxbpRmGwvaN",
    "outputId": "9315acb4-6b3a-48f0-927a-0cdc4fb1f6b2"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,7))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.title('Лосс', fontsize=15)\n",
    "plt.plot(history['loss']['train'], label='train')\n",
    "plt.plot(history['loss']['val'], label='val')\n",
    "plt.xlabel('эпоха', fontsize=15)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('Точность', fontsize=15)\n",
    "plt.plot(history['acc']['train'], label='train')\n",
    "plt.plot(history['acc']['val'], label='val')\n",
    "plt.xlabel('эпоха', fontsize=15)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Up7Q1fQwwq5K"
   },
   "source": [
    "**Заметки**\n",
    "* Не забывайте **занулять градиенты** после каждой итерации.\n",
    "* Если ваш loss стал `nan`/`inf`, то повыводите то, что происходит на каждой итерации и поймите в каком именно месте проблема.\n",
    "* Если ваш loss уменьшался, а потом стал равен `nan`, то попробуйте уменьшить `learning rate`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3R3gWMWxFD3"
   },
   "source": [
    "Сделаем аналогичную модель, только с 3 слоями, вместо 2, и в виде модуля, а не Sequential-модели. Заметим, что здесь наша модель - это модуль, который наследуется от `nn.Module` и содержит в себе другие модули, такие как `nn.Linear`.\n",
    "\n",
    "Как было сказано выше, модуль должен обязательно иметь метод `forward()`, который мы сами определяем. Метод `backward()`является необязательным, PyTorch сможет сам понять, что делать при обратном проходе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JfJ__vpsxHr3"
   },
   "outputs": [],
   "source": [
    "class MySimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Здесь объявляем все слои, которые будем использовать\n",
    "        '''\n",
    "        super(MySimpleModel, self).__init__()\n",
    "        self.linear1 = nn.Linear(3 * 32 * 32, 256)\n",
    "        self.linear2 = nn.Linear(256, 64)\n",
    "        self.linear3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Здесь пишем в коде, в каком порядке какой слой будет применяться\n",
    "        '''\n",
    "        x = self.linear1(nn.Flatten()(x))\n",
    "        x = self.linear2(nn.ReLU()(x))\n",
    "        x = self.linear3(nn.ReLU()(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nOnwIErhxLO5"
   },
   "outputs": [],
   "source": [
    "###### 1. Обернем цикл обучения нейросети в отдельную функцию ######\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    X_train, y_train,\n",
    "    X_val, y_val,\n",
    "    num_epochs=30,\n",
    "    batch_size=64,\n",
    "    model_path='model.pth'\n",
    "):\n",
    "    \"\"\"\n",
    "    # Обучение модели\n",
    "    \"\"\"\n",
    "\n",
    "    num_train_batches = len(X_train) // batch_size\n",
    "    num_val_batches = len(X_val) // batch_size\n",
    "\n",
    "    history = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    best_val_acc = 0.\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        val_loss = 0\n",
    "        val_acc = 0\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        model.train(True)  # устанавливаем поведение dropout / batch_norm  в обучение\n",
    "\n",
    "        # На каждой \"эпохе\" делаем полный проход по данным\n",
    "        for X_batch, y_batch in batch_generator(X_train, y_train, batch_size, device=device):\n",
    "\n",
    "            # Обучаемся на батче (одна \"итерация\" обучения нейросети)\n",
    "            logits = model(X_batch)\n",
    "\n",
    "            loss = criterion(logits, y_batch)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            train_loss += loss.detach().cpu().numpy()\n",
    "            y_pred_np = np.argmax(logits.detach().cpu().numpy(), axis=1)\n",
    "            y_batch_np = y_batch.cpu().numpy()\n",
    "            train_acc += (y_batch_np == y_pred_np).sum()\n",
    "\n",
    "        # Подсчитываем лоссы и сохраням в \"историю\"\n",
    "        train_loss /= num_train_batches\n",
    "        train_acc /= num_train_batches * batch_size\n",
    "        history['loss']['train'].append(train_loss)\n",
    "        history['acc']['train'].append(train_acc)\n",
    "\n",
    "        # Устанавливаем поведение dropout / batch_norm  в тестирование\n",
    "        model.eval()\n",
    "\n",
    "        # Полный проход по валидации\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in batch_generator(X_val, y_val, batch_size, device=device):\n",
    "                logits = model(X_batch)\n",
    "                loss = criterion(logits, y_batch)\n",
    "\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                y_pred_np = np.argmax(logits.cpu().numpy(), axis=1)\n",
    "                y_batch_np = y_batch.cpu().numpy()\n",
    "                val_acc += (y_batch_np == y_pred_np).sum()\n",
    "\n",
    "        # Подсчитываем лоссы и сохраням в \"историю\"\n",
    "        val_loss /= num_val_batches\n",
    "        val_acc /= num_val_batches * batch_size\n",
    "        history['loss']['val'].append(val_loss)\n",
    "        history['acc']['val'].append(val_acc)\n",
    "\n",
    "        # Сохраняем лучшую модель по метрике на валидации\n",
    "        if val_acc > best_val_acc:\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "        # Печатаем результаты после каждой эпохи\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss (in-iteration): \\t{:.6f}\".format(train_loss))\n",
    "        print(\"  validation loss (in-iteration): \\t{:.6f}\".format(val_loss))\n",
    "        print(\"  training accuracy: \\t\\t\\t{:.2f} %\".format(train_acc * 100))\n",
    "        print(\"  validation accuracy: \\t\\t\\t{:.2f} %\".format(val_acc * 100))\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HOLi7B3cxPBX"
   },
   "outputs": [],
   "source": [
    "###### 2. Обернем построение графиков в отдельную функцию    ######\n",
    "\n",
    "def plot_learning_curves(history):\n",
    "    \"\"\"\n",
    "    Построение графиков\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(20,7))\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title('Лосс', fontsize=15)\n",
    "    plt.plot(history['loss']['train'], label='train')\n",
    "    plt.plot(history['loss']['val'], label='val')\n",
    "    plt.xlabel('эпоха', fontsize=15)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title('Точность', fontsize=15)\n",
    "    plt.plot(history['acc']['train'], label='train')\n",
    "    plt.plot(history['acc']['val'], label='val')\n",
    "    plt.xlabel('эпоха', fontsize=15)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5afDGksvxSYw",
    "outputId": "494b6caa-8864-418e-afd2-eb9b140cabd8"
   },
   "outputs": [],
   "source": [
    "###### 3. Обучим модель в Functional-стиле на CIFAR10     ######\n",
    "\n",
    "model = MySimpleModel().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "model, history = train(\n",
    "    model, criterion, optimizer,\n",
    "    X_train, y_train,\n",
    "    X_val, y_val,\n",
    "    num_epochs=30,\n",
    "    batch_size=50,\n",
    "    model_path='simple_model.pth'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 649
    },
    "id": "AnKTcnwxxUfW",
    "outputId": "c9d0a87e-402c-4c36-b5c0-5addb2227ea1"
   },
   "outputs": [],
   "source": [
    "plot_learning_curves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EVju1O9lHp0e",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Теперь рассмотрим как получить генератор батчей методами PyTorch. Как ранее уже упоминалось, для этого используется `DataLoader`\n",
    "\n",
    "```\n",
    "DataLoader(dataset, batch_size=1, shuffle=False, sampler=None,  \n",
    "           batch_sampler=None, num_workers=0, collate_fn=None,  \n",
    "           pin_memory=False, drop_last=False, timeout=0,  \n",
    "           worker_init_fn=None, *, prefetch_factor=2,  \n",
    "           persistent_workers=False)\n",
    "```\n",
    "Рассмотрим параметры, которые нам сейчас пригодятся.\n",
    "* `dataset` &mdash; датасет, на котором будет построен итератор. Каким он должен быть, будет пояснено далее.\n",
    "\n",
    "* `batch_size` &mdash; размер батча.\n",
    "\n",
    "* `shuffle` &mdash; стоит ли перемешивать данные до разделения на батчи. Параметр имеет смысл, если не указан способ семплирования с помощью параметров `sampler` или `batch_sampler`. Если параметр задан как `True`, то выборка будет перемешана заново до наступления нового цикла итерирования. При этом батчи будут выбираться последовательно из перемешанной выборки.\n",
    "\n",
    "* `num_workers` &mdash; с помощью этого параметра можно установить параллельную загрузку данных. Он обозначает количество процессов, которые будут запущены в мульти-процессорном режиме.\n",
    "\n",
    "Как мы видим, для `DataLoader` нужен еще `dataset`. Это может быть map-style или iterable датасет. Мы зададим iterable. Для этого мы создадим класс, у которого есть методы `__getitem__` и `__len__`.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MhXNpgNeHp0e",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class SimpleDataset():\n",
    "    \"\"\"\n",
    "    Простой итерируемый датасет.\n",
    "    \"\"\"\n",
    "    def __init__(self, X_data, y_data):\n",
    "        assert len(X_data) == len(y_data)\n",
    "        self.x = X_data\n",
    "        self.y = y_data\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Размер датасета.\n",
    "        \"\"\"\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Обращение к элементу датасета по индексу.\n",
    "        \"\"\"\n",
    "        xi = self.x[idx].ravel().astype(np.float32)\n",
    "        yi = self.y[idx].astype(np.longlong)\n",
    "        return xi, yi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xqRgPKFHp0e",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Инициализируем датасеты и итераторы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z_xPXUb_Hp0e",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yz7VrIl6cqnk",
    "outputId": "e5f29e66-c9f3-499f-f493-bae4cd964556"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "num_workers = os.cpu_count()\n",
    "print(f\"Количество доступных num_workers: {num_workers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BAOjfeqVHp0e",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "fc5f8415-acae-4f12-ca70-583777cafc8e"
   },
   "outputs": [],
   "source": [
    "# Обучающий и валидационный датасеты\n",
    "train_dataset = SimpleDataset(X_train, y_train)\n",
    "val_dataset = SimpleDataset(X_val, y_val)\n",
    "\n",
    "# Размер батча\n",
    "batch_size = 32\n",
    "\n",
    "# Количество процессов в dataloader\n",
    "num_workers = 6\n",
    "\n",
    "# Обучающий и валидационный генераторы данных\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size,\n",
    "    shuffle=True, num_workers=num_workers\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size,\n",
    "    shuffle=False, num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_2qydxqHp0e",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Проверим, что все работает. `DataLoader` сам привел NumPy массивы к PyTorch тензорам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJ5945bNHp0f",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "4790072b-4b17-408f-f402-495bf98b2ccc"
   },
   "outputs": [],
   "source": [
    "x_batch, y_batch = next(iter(train_loader))\n",
    "x_batch.shape, y_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0xbOzipHp0f",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Запишем цикл обучения в виде функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DAYH1NGuHp0f",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    num_epochs,\n",
    "    device,\n",
    "    checkpoint_path='model.pt',\n",
    "):\n",
    "    \"\"\"\n",
    "    # Обучение модели\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    history = defaultdict(lambda: defaultdict(list))\n",
    "    best_val_acc = 0.\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        val_loss = 0\n",
    "        val_acc = 0\n",
    "        train_size, val_size = 0, 0\n",
    "\n",
    "        model.train()  # устанавливаем поведение dropout / batch_norm  в обучение\n",
    "\n",
    "        # На каждой \"эпохе\" делаем полный проход по данным\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            # Обучаемся на батче (одна \"итерация\" обучения нейросети)\n",
    "            logits = model(X_batch)\n",
    "            loss = criterion(logits, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            train_loss += loss.detach() * len(y_batch)\n",
    "            y_pred = torch.argmax(logits, axis=1).detach()\n",
    "            train_acc += (y_batch == y_pred).sum()\n",
    "            train_size += len(y_batch)\n",
    "\n",
    "        # Подсчитываем лоссы и точность, сохраняем в \"историю\"\n",
    "        train_loss = train_loss.cpu().numpy() / train_size\n",
    "        train_acc = train_acc.cpu().numpy() / train_size\n",
    "        history['loss']['train'].append(train_loss)\n",
    "        history['acc']['train'].append(train_acc)\n",
    "\n",
    "        # Устанавливаем поведение dropout / batch_norm  в тестирование\n",
    "        model.eval()\n",
    "\n",
    "        # Полный проход по валидации\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                logits = model(X_batch)\n",
    "                loss = criterion(logits, y_batch)\n",
    "\n",
    "                val_loss += loss.detach() * len(y_batch)\n",
    "                y_pred = torch.argmax(logits, axis=1).detach()\n",
    "                val_acc += (y_batch == y_pred).sum()\n",
    "                val_size += len(y_batch)\n",
    "\n",
    "        # Подсчитываем лоссы и сохраняем в \"историю\"\n",
    "        val_loss = val_loss.cpu().numpy() / val_size\n",
    "        val_acc = val_acc.cpu().numpy() / val_size\n",
    "        history['loss']['val'].append(val_loss)\n",
    "        history['acc']['val'].append(val_acc)\n",
    "\n",
    "        # Сохраняем лучшую модель по метрике на валидации\n",
    "        if val_acc > best_val_acc:\n",
    "            torch.save({\n",
    "                'model': model.state_dict(),\n",
    "                'optim': optimizer.state_dict(),\n",
    "                }, checkpoint_path)\n",
    "\n",
    "        clear_output()\n",
    "        # Печатаем результаты после каждой эпохи\n",
    "        print(\"Эпоха {} из {}, время {:.3f}s\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time\n",
    "        ))\n",
    "        print(\"  loss на обучении (на итерации): \\t{:.6f}\".format(train_loss))\n",
    "        print(\"  loss на валидации (на итерации): \\t{:.6f}\".format(val_loss))\n",
    "        print(\"  accuracy на обучении: \\t\\t\\t{:.2f} %\".format(train_acc * 100))\n",
    "        print(\"  accuracy на валидации: \\t\\t\\t{:.2f} %\".format(val_acc * 100))\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i4GvlWN_2WGP"
   },
   "source": [
    "**ЗАДАНИЕ 5**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KlUkjnbDH7BN"
   },
   "source": [
    "Поменяйте функцию активации поcле первого слоя на любую другую"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5DU5nDmifKED"
   },
   "outputs": [],
   "source": [
    "class Model_with_Sigmoid(nn.Module):\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Здесь объявляем все слои, которые будем использовать\n",
    "        '''\n",
    "        super(Model_with_Sigmoid, self).__init__()\n",
    "        self.linear1 = nn.Linear(3 * 32 * 32, 256)\n",
    "        self.linear2 = nn.Linear(256, 64)\n",
    "        self.linear3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Здесь пишем в коде, в каком порядке какой слой будет применяться\n",
    "        '''\n",
    "        x = self.linear1(nn.Flatten()(x))\n",
    "        x = self.linear2(nn.Sigmoid()(x))\n",
    "        x = self.linear3(nn.ReLU()(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnvQ6pPoH7BO"
   },
   "source": [
    "Обучите модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SBWeM-oHH7BN",
    "outputId": "7d79a710-463f-4442-a59e-96de3764c696"
   },
   "outputs": [],
   "source": [
    "model = Model_with_Sigmoid().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "history = train(\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    num_epochs=30,\n",
    "    device=device,\n",
    "    checkpoint_path='model_with_sigmoid.pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2BGXT4N3Y2j"
   },
   "source": [
    "Оцените качество"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 649
    },
    "id": "b20dXVrw3bUd",
    "outputId": "773ffb25-f5bb-49f3-c241-2fafc4d71d36"
   },
   "outputs": [],
   "source": [
    "plot_learning_curves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWvieepLH7BO"
   },
   "source": [
    "Что изменилось в процессе обучения? Стало ли предсказание лучше?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4T-0VjfRgzAe"
   },
   "source": [
    "**Выводы:**\n",
    "\n",
    "**Лосс:** Мы видим, что изменение функции активации привело к более медленному снижению лосса на обучении и валидации. Это связано с тем, что сигмоида может насыщаться (из-за экспоненциальной функции), что приводит к \"заглушке\" градиентов и снижению способности модели учиться.\n",
    "\n",
    "**Точность:** На графике точности видно, что модель также медленнее учится, и валидационная точность увеличивается медленнее по сравнению с предыдущей моделью. Точность на валидации достигает около 45%, что примерно на том же уровне, что и у базовой модели.\n",
    "\n",
    "**ИТОГ:**\n",
    "\n",
    "Сигмоида вызывает \"затухание градиентов\", особенно при активациях, близких к 0 или 1. Это привело к тому, что модель учится медленнее, и снижение лосса на обучении и валидации замедлилось. Модель не смогла достичь лучших результатов по сравнению с ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "phz-nq2BH7BO"
   },
   "source": [
    "\n",
    "**ЗАДАНИЕ 6**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QlDsavJh3mHq"
   },
   "source": [
    "Измените количество признаков на скрытом слое"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qg-9YQufH7BP"
   },
   "outputs": [],
   "source": [
    "class Model_another_layers(nn.Module):\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Здесь объявляем все слои, которые будем использовать\n",
    "        '''\n",
    "        super(Model_another_layers, self).__init__()\n",
    "        self.linear1 = nn.Linear(3 * 32 * 32, 128)\n",
    "        self.linear2 = nn.Linear(128, 64)\n",
    "        self.linear3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Здесь пишем в коде, в каком порядке какой слой будет применяться\n",
    "        '''\n",
    "        x = self.linear1(nn.Flatten()(x))\n",
    "        x = self.linear2(nn.ReLU()(x))\n",
    "        x = self.linear3(nn.ReLU()(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Belo2F-w5HAJ"
   },
   "source": [
    "Обучите модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S1MNSer65HAL",
    "outputId": "1f7b96cb-e899-4624-bb94-fc53984d2091"
   },
   "outputs": [],
   "source": [
    "model = Model_another_layers().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "history = train(\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    num_epochs=30,\n",
    "    device=device,\n",
    "    checkpoint_path='model_another_layers.pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APAGHSH75HAM"
   },
   "source": [
    "Оцените качество"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 649
    },
    "id": "EUcFpr7e5HAM",
    "outputId": "417d61d1-ae53-4016-8deb-7722e4134949"
   },
   "outputs": [],
   "source": [
    "plot_learning_curves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_zBhKxSJH7BP"
   },
   "source": [
    "С чем может быть связано изменение качества?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKKLp4dsuUUI"
   },
   "source": [
    "**Выводы:**\n",
    "\n",
    "**Лосс:** Линии лосса на обучении и валидации хорошо сбалансированы. Лосс на валидации показывает небольшие колебания, но это нормально при более сложных признаках.\n",
    "\n",
    "**Точность:** Точность на валидации продолжает расти, хотя с колебаниями. Максимальная точность достигает 51%, что лучше, чем в базовой модели. Это указывает на то, что изменение признаков на скрытом слое улучшает результат.\n",
    "\n",
    "**ИТОГ:**\n",
    "\n",
    "Да, изменение признаков на скрытом слое улучшило результат. Точность на валидации возросла до 51%, что выше по сравнению с базовой моделью. Это говорит о том, что правильная настройка скрытых слоев может положительно повлиять на обучение модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jqGQMnpw4hr1"
   },
   "source": [
    "# DropOut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FHx8j9kJ4mRz"
   },
   "source": [
    "Исключение или дропаут (от англ. dropout) — метод регуляризации искусственных нейронных сетей, предназначен для уменьшения переобучения сети за счет предотвращения сложных коадаптаций отдельных нейронов на тренировочных данных во время обучения.\n",
    "\n",
    "Термин «dropout» (выбивание, выбрасывание) характеризует исключение определённого процента (например 30 %) случайных нейронов (находящихся как в скрытых, так и видимых слоях) на разных итерациях (эпохах) во время обучения нейронной сети. Это очень эффективный способ усреднения моделей внутри нейронной сети. В результате более обученные нейроны получают в сети больший вес. Такой приём значительно увеличивает скорость обучения, качество обучения на тренировочных данных, а также повышает качество предсказаний модели на новых тестовых данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XuLZljA56kAe"
   },
   "source": [
    "После выключения какого-то процента нейронов слоя, неоюодимо перенормировать выходы этого слоя, поделив их на вероятность дропаута, чтобы сохранить распределение выходов. На инференсе ничего делать не надо, так как активированы все нейроны"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RZ8li_9wMVzB"
   },
   "source": [
    "Как вы уже догадываетесь, PyTorch все реализовал за нас. Для применения дропаута нужно дабавить слой `nn.Dropout()` в вашу модель."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RqCmq2ICQ5Nw"
   },
   "source": [
    "**ЗАДАНИЕ 7**\n",
    "\n",
    "Обучите сеть при разных значениях вероятности отключения нейронов (не менее 3 разных значений). Попробуйте улучшить имеющийся результат. Объясните полученные вами значения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_KATHrJhb7zA"
   },
   "outputs": [],
   "source": [
    "class Model_DropOut(nn.Module):\n",
    "    def __init__(self, dropout_prob = 0.5):\n",
    "        '''\n",
    "        Здесь объявляем все слои, которые будем использовать\n",
    "        '''\n",
    "        super(Model_DropOut, self).__init__()\n",
    "        self.linear1 = nn.Linear(3 * 32 * 32, 256)\n",
    "        self.linear2 = nn.Linear(256, 64)\n",
    "        self.linear3 = nn.Linear(64, 10)\n",
    "        self.dropout = nn.Dropout(p = dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Здесь пишем в коде, в каком порядке какой слой будет применяться\n",
    "        '''\n",
    "        x = self.linear1(nn.Flatten()(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(nn.ReLU()(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear3(nn.ReLU()(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "95DChMhIhg-w",
    "outputId": "380cc777-7c50-413a-861f-2d4793ef4204"
   },
   "outputs": [],
   "source": [
    "dropout_probs = [0.3, 0.5, 0.7]  # Пробуем разные значения dropout\n",
    "for prob in dropout_probs:\n",
    "    print(f\"Тестируем с DropOut c вероятностью: {prob}\")\n",
    "\n",
    "    model = Model_DropOut(dropout_prob=prob).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "    history = train(\n",
    "        model,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        num_epochs = 30,\n",
    "        device=device,\n",
    "        checkpoint_path = f'model_dropout_{prob}.pt'\n",
    "    )\n",
    "\n",
    "    plot_learning_curves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mnseXUYuk1q"
   },
   "source": [
    "**Выводы:**\n",
    "\n",
    "**Loss:**\n",
    "\n",
    "* **Dropout 0.3:** Лосс на обучении и валидации стабильно уменьшается.Валидационная кривая чуть более неровная, но в целом модель хорошо учится.\n",
    "\n",
    "* **Dropout 0.5:** Графики лосса показывают небольшие колебания на валидации, однако модель продолжает улучшаться.\n",
    "\n",
    "* **Dropout 0.7:** Валидационный лосс значительно колеблется. Возможно, слишком высокая вероятность Dropout привела к тому, что модель учится слишком медленно или \"сбрасывает\" слишком много информации.\n",
    "\n",
    "**Accuracy:**\n",
    "\n",
    "* **Dropout 0.3:** Точность на валидации около 46%, что чуть хуже базовой модели.\n",
    "\n",
    "* **Dropout 0.5:** Точность на валидации максимальная среди всех моделей с Dropout, около 48%. Этот вариант кажется оптимальным.\n",
    "\n",
    "* **Dropout 0.7:** Модель демонстрирует худшую точность среди всех вариантов Dropout — около 44%. Это указывает на то, что Dropout с вероятностью 0.7 слишком сильно \"отключает\" нейроны, и модель теряет много информации.\n",
    "\n",
    "**ИТОГ:**\n",
    "\n",
    "Dropout с вероятностью 0.5 дал лучший результат с точностью около 48%. Этот вариант обеспечил наилучший баланс между регуляризацией и сохранением информации в сети."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7QX3u0t9vJgA"
   },
   "source": [
    "**ИТОГИ:**\n",
    "\n",
    "* Замена функции активации на сигмоиду замедлила обучение.\n",
    "* Изменение признаков на скрытом слое улучшило качество предсказаний.\n",
    "* Dropout с вероятностью 0.5 оказался наилучшим выбором для данной задачи."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
